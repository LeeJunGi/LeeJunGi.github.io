---
title: 6. DQN 
author: Jungi Lee
date: 2021-04-18 21:30:00 +0900
categories: [Reinforcement Learning, Single-Agent]
tags: [RL, Single-Agent]
math: true
mermaid: true
---
# 목차 
1. [13DQN](#13dqn)  
1. [15DQN](#15dqn)  
1. [Reference](#reference)  

# 13DQN

Atari 게임과 같이 state가 많은 곳에서 Q-table을 사용하기가 어렵기 때문에 DNN을 이용하여 해결한다. DNN의 output이 Q값이 나오고 Q의 1 step TD를 이용하여 network parameter를 update한다. 

13 DQN의 contribution은 3가지 이다.  
1. CNN 이용  
1. Frame skipping  
	- 게임에서 바로 다음 frame은 이전 frame과 매우 비슷한 값을 가지기 때문에 state correlation이 높다는 문제가 있다.  
	- Frame skipping을 통하여 4 frame을 같은 action을 사용한다.
1. Replay buffer  
	- 현재까지 만들어진 trajectory를 replay buffer라는 메모리 구조를 만들어 저장을 하고 random batch로 불러와서 DNN을 학습시킨다.  
	- Q-learning은 off policy이기 때문에 가능하다.  

13 DQN의 알고리즘 아래와 같다. 
![13DQN alg][13DQN alg]
_13DQN algorithm_

# 15DQN
13 DQN은 target value가 계속 변하여 network training이 힘들다는 문제가 있다. target network와 main network를 두어서 학습해서 해결하는 것을 제안한다. DNN update를 위한 TD target은 target network로 만든다. 이 TD target을 이용해 main network를 update한다. main network만 학습하고 target network는 학습하지 않기 때문에 TD target은 변하기 않는다. 이후 C step 이후 target network의 parameter를  main network의 parameter로 대입하여 update해준다.  

15 DQN Algorithm

$$ y_j = \begin{cases} r_j \qquad \text{if episode terminates at step j+1} \\ r_j + \gamma \text{max}_d \hat{Q}(\phi_{j+1},a^{'};\theta^{-}) \qquad \text{otherwise} \end{cases}$$

Gradient descent step on $$(y_j - Q(\phi_j,a_j;\theta))^2$$ with respect to the network parameters $$\theta$$  
Every C steps reset $$\hat{Q} = Q$$



# Reference
1. [혁펜하임 유투브][혁펜하임 유투브]  
1. [Mnih, Volodymyr, et al. "Playing atari with deep reinforcement learning." arXiv preprint arXiv:1312.5602 (2013).][13DQN]  
1. [Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." nature 518.7540 (2015): 529-533.][15DQN]  

[혁펜하임 유투브]: https://www.youtube.com/watch?v=cvctS4xWSaU&list=PL_iJu012NOxehE8fdF9me4TLfbdv3ZW8g  
[13DQN]: https://arxiv.org/pdf/1312.5602.pdf
[15DQN]: https://www.nature.com/articles/nature14236?wm=book_wap_0005

[13DQN alg]: /assets/img/Single-agent/DQN/13DQN_alg.png



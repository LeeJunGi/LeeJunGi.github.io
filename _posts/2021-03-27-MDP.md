---
title: 1. Markov Decision Process 
author: Jungi Lee
date: 2021-03-27 20:00:00 +0900
categories: [Reinforcement Learning, Single-Agent]
tags: [RL, Single-Agent]
math: true
mermaid: true
---
# 목차
[1. 시작하기 전에](#시작하기-전에)  
[2. Markov Process](#markov-processmp)  
[3. Markov Decision Process](#markov-deicison-processmdp)  
[4. Markov chain](#markov-chain)  
[5. 용어 정리](#용어-정리)  
[6. Goal](#goal)

# 시작하기 전에
Reinforcement Learning(RL)을 공부하기 위해서 여러 책, 자료를 찾아봤고 이해하는 데 가장 큰 도움이 됬던 강의는 유투브 [혁펜하임](https://www.youtube.com/watch?v=cvctS4xWSaU&list=PL_iJu012NOxehE8fdF9me4TLfbdv3ZW8g)이라는 채널에서 만든 강화학습 강의였다. 이 single-agent 시리즈 포스팅은 해당 유투브를 통해 얻은 지식을 바탕으로 정리를 할 것이다. 하지만 이 강의와 책들의 표기 방식이 다르기 때문에 책들의 표기 방식도 고려하여 같이 정리를 할 예정이다.(강의는 integral형식, 책은 Expectation 형식으로 많이 적혀있던 것으로 기억하고 있다.) 
>다른 사람들과 함께 공부하고 전문가에게 배운 것이 아닌 책과 동영상으로만 독학했기 때문에 잘못된 설명과 방식이 있을 수 있습니다. 이에 대해서는 이메일을 주시거나 아직 없지만 댓글이 활성화된다면 댓글을 달아주시면 감사하겠습니다.

# Markov Process(MP)
RL은 기본적으로 MDP로 표현한다. 그렇다면 Markov Deicison Process(MDP)는 무엇인가? MDP를 알기전에 Markov Process(MP)를 알고 가자.

MP는 시간이 진행함에 따라 상태가 확률적으로 변화하는 과정이다. 어떤 확률 분포를 따르는 random variable이 discrete한 time interval마다 값을 생성하는 것을 의미한다. 즉 시간에 따른 상태 변화를 확률로 표현된다는 것같다. 
<center>$$P(s=s_{i+1}|s=s_i)$$</center>

state(상태)가 두 두가지가 있다면 MP는 s1 -> s1, s1 -> s2, s2 -> s1, s2 -> s2와 같이 네 개를 표현할 수 있도록 만들어질 것이다.
<center>$$P(s=s_1|s=s_1)$$</center>
<center>$$P(s=s_2|s=s_1)$$</center>
<center>$$P(s=s_1|s=s_2)$$</center>
<center>$$P(s=s_2|s=s_2)$$</center>

# Markov Deicison Process(MDP)
MDP는 MP에서 reward와 action이 주어진 것이라고 생각하면 좋을 것같다. 현재 $$s_i$$ 상태에서 $$a_i$$의 action을 하였을 때 $$r_i$$의 reward을 얻을 수 있고 $$s_{i+1}$$로 상태가 변화될 것이다. 

# Markov chain
MDP의 특별한 점 중 하나이다. MDP는 미래의 상태는 현재의 상태에만 영향을 받고 과거의 상태에서는 영향을 받지 않는다는 특징이 있다. 그래서 $$s_{i1}$$의 정보를 얻기 위해서 현재 $$s_i$$에서 $$a_i, s_{i+1}, a_{i+1}$$의 과정이 필요하다. 하지만 Markov chain으로 인하여 $$s_{i+1}$$에는 이미 $$s_i, a_i$$의 정보가 있으므로 식에서 제거가 가능하다. 그 결과 아래와 같은 공식이 성립하게 된다.
<center>$$P(s=s_{i+2}|s=s_{i+1},a=a_{i+1})=P(s=s_{i+2}|s=s_i,a=a_i,s=s_{i+1},a=a_{i+1})$$</center> 
<center>$$P(s_{i+2}|s_{i+1},a_{i+1})=P(s_{i+2}|s_i,a_i,s_{i+1},a_{i+1})$$</center> 

# 용어 정리
RL에서 policy, return, transition probability, state value function, action value function등의 용어를 자주 사용한다.

- **Policy**: state s에서 action a를 취할 확률
<center>$$P(s=s_{i+1}|s=s_i,a=a_i)$$</center> 
- **Transition**: state s에서 action a를 해서 state s'으로 전이될 확률 
- **Reward**: state s에서 action a를 취하며 발생한 보상
- **Return**: 한 episode를 진행하면서 얻을 수 있는 보상들의 합
<center>$$G_t \doteq R_t + \gamma R_{t+1} + \gamma ^2 R_{t+2} + \cdots$$</center>
- **State value function**: 지금부터 기대되는 return, 현재 state 이후부터 기대되는 return을 의미  
\* \doteq means definition
<center>$$E[f(x)] = \int f(x)g(x) \,dx$$</center>
<center>$$V(s_t) \doteq \int_{a_{t}:a_{\infty}} G_tP(a_{t+1},s_{t+2},a_{t+2} \cdots |s_t) \, da_t:a_{\infty}$$</center>  
<center>$$V(s_t) \doteq E_{\pi}[\sum_{k=0}^\infty \gamma^k R_{t+k+1} |S_t=s] = E_{\pi}[G_t|S_t=s]$$</center>  
- **Action value function**: 지금 행동으로부터 기대되는 return
<center>$$Q(s_t,a_t) \doteq \int_{s_{t+1}:a_{\infty}} G_tP(s_{t+1},a_{t+1},s_{t+2} \cdots |s_t,a_t) \, ds_{t+1}:a_{\infty}$$</center>
<center>$$Q(s_t,a_t) \doteq E_{\pi} [G_t|S_t=s,A_t=a]$$</center>

# Goal
강화학습의 목표는 Expetecd return을 최대화하는 것에 있다.
> Find maximum <span style="color:red">**Expeted Return**</span>



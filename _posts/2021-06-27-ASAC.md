---
title: (Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications
author: Jungi Lee
date: 2021-06-27 01:50:00 +0900
categories: [Reinforcement Learning, Single-Agent]
tags: [RL, Single-Agent]
math: true
mermaid: true
---
# 목차 
1. [Introduction](#introduction)  
1. [Automating Entropy Adjustment for Maximum Entropy RL](#automating-entropy-adjustment-for-maximum-entropy-rl)  
1. [Algorithm](#algorithm)  
1. [Reference](#reference)  

# Introduction

이전 논문인 SAC에서 문제인 고정된 temperature의 문제를 해결하는 부분이 추가된 SAC base논문이다. optimal temperature를 찾는 문제는 쉽지 않은 문제이다. 이를 고정해서 사용하게 된다면 학습 결과가 안 좋게 나올 수 있다. 그래서 optimal action의 uncertainty에 따라 temperature를 유동적으로 변경해서 사용해야한다. 이 문제를 해결하기 위해 최적화 목적식에서 entropy term을 constraint로 설정하였다.

# Automating Entropy Adjustment for Maximum Entropy RL

SAC와 다르게 목적식의 entropy term이 constraint로 설정하였다.

$$\underset{\pi_{0:T}}{\text{max}}E_{\rho_\pi} [\underset{t=0}{\overset{T}{\sum}} r(s_t, a_t)]$$

subject to $$E_{(s_t,a_t) \sim \rho_\pi} [-\text{log}(\pi_t(a_t \vert s_t))] \ge H$$

그리고 MDP에따라 time t의 policy는 미래의 value에만 영향을 주기 때문에 dynamic programming approach를 사용할 수 있다. 

$$\text{max}_{\pi_0}(E[r(s_0, a_0)] + \text{max}_{\pi_1} (E[\dots] + \text{max}_{\pi_T}E[r(s_T,a_T)]))$$


objective function이 linear하고 constraint가 convex function이기 때문에 최적화가 가능하다. 그래서 optimal policy는 temperature에 따른 maximum entropy policy를 찾는 것이다. optimal daul variable $$a^*_T$$를 아래식처럼 풀 수 있다.

$$\text{argmin}_{a_T} E_{s_t, a_t \sim \pi^*_t} [-\alpha_T \text{log} \pi^*_T (a_T \vert s_T; \alpha_T) - \alpha_T H]$$

soft Q-function은 아래식으로 표현이 가능하다.

$$Q_{T-1}(s_{T-1}, a_{T-1}) = E[r(s_{T-1}, a_{T-1})] + E[Q(s_T, a_T) - \alpha_T \text{log} \pi (a_T \vert s_T)]$$
$$=E[r(s_{T-1}, a_{T-1})] + E[r(s_T, a_T)] + \alpha_T H(\pi_T)$$

$$Q^*_{T-1}(s_{T-1}, a_{T-1}) = E[r(s_{T-1}, a_{T-1})] + \text{max}E[r(s_T, a_T)] +\alpha^*_T H(\pi^*_T)$$

Recursive하게 표현되는 목적식에서 반복되는 부분을 최적화하면 전체를 최적화한다는 dynamic programming 방법을 적용하였으므로 아래식을 최적화 하는 것이 결국 목적식을 최적화한다는 뜻이다.

$$\underset{\pi_{T-1}}{\text{max}} (E[r(s_{T-1}, a_{T-1})] + \underset{\pi_T}{\text{max}}E[r(s_T,a_T)])$$

$$=\underset{\pi_{T-1}}{\text{max}} (Q^*_{T-1}(s_{T-1}, a_{T-1}) - \alpha_T H)$$

$$ = \underset{\alpha_{T-1} \ge 0}{\text{min}} \underset{\pi_{T-1}}{\text{max}}(E[Q^*_{T-1}(s_{T-1}, a_{T-1}] - E[\alpha_{T-1} \text{log} \pi (a_{T-1} \vert s_{T-1})] - \alpha_{T-1} H) + \alpha^*_T H$$

이 식을 최적화 하면 결국 optimal temperature를 고려하며 policy gradient가 된다. 위 식을 $$\alpha_{T-1}$$에 대해서 미분을 하여 optimal $$\alpha$$를 찾을 수 있다.
	
# Algorithm

![algorithm][algo]
_Algorithm_

이전 SAC 논문에서와 다르게 Value function을 사용하지 않는다. 그리고 value function에서 사용하였던 soft update와 target network를 Q function에 적용하였다. 그리고 2 soft Q-function이 training 단계에서 속도가 더 오르는 것을 확인하였다고 한다.

이 논문의 추가적인 부분인 automating entropy adjustment가 추가되었다.


# Reference
1. [Haarnoja, Tuomas, et al. "Soft actor-critic algorithms and applications." arXiv preprint arXiv:1812.05905 (2018).][paper]
2. [Description about SAC with automatically adjusted temperature][ref]

[paper]: https://arxiv.org/pdf/1812.05905.pdf 
[ref]: https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#sac-with-automatically-adjusted-temperature

[algo]: /assets/img/Single-agent/ASAC/algo.png


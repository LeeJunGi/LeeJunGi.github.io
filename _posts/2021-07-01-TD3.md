---
title: (Fujimoto 2018 ICML) Addressing Function Approximation Error in Actor-Critic Methods
author: Jungi Lee
date: 2021-07-01 01:40:00 +0900
categories: [Reinforcement Learning, Single-Agent]
tags: [RL, Single-Agent]
math: true
mermaid: true
---
# 목차 
1. [Introduction](#introduction)  
1. [Clipped Double Q-Learning for Actor-Critic](#clipped-double-q-learning-for-actor-critic)  
1. [Target Networks and Delayed Policy Updates](#target-networks-and-delayed-policy-updates)  
1. [Target Policy Smoothing Regularization](#target-policy-smoothing-regularization)  
1. [Algorithm](#algorithm)  
1. [Reference](#reference)  

# Introduction

이 논문은 DDPG base로 발전한 논문이다. 이전에 많이 발생하였던 over estimation 문제를 해결하기 위해 3가지 방법을 제시한다. overestimation bias 문제는 suboptimal policy로 수렴하거나 policy가 발산할 수 있으므로 중요한 부분이다. 이 논문에서는 clipped Double Q learning, delay policy update, SARSA style update bootstrap 기법을 이용한다.

# Clipped Double Q-Learning for Actor-Critic

overestimation으로 인하여 error가 축적될 경우 bias가 생겨 안 좋은 policy를 생성하게 된다. 그래서 이전에 Double Q-learning이라는 방법으로 해결을 하자고 했지만 부족한 부분이 있다. 그래서 clipped variant를 이용하여 개선하고자 한다. 

이 논문 실험을 통하여 이전 방법은 천천히 개선되고 current와 target network가 너무 비슷하여 독립적인 결과를 생성하기가 힘들어 천천히 개선이 된다. 

$$Q_{\theta_1}$$을 이용하여 $$Q_{\theta_2}$$를 덜 biased되도록 upper-bound되게 만든다. 이 방법은 2개의 target에 대해서 최소 값을 사용하여 estimation을 한다.

$$y_1 = r + \gamma \underset{i=1,2}{\text{min}} Q_{\theta_i '} (s', \pi_{\phi_i} (s'))$$

# Target Networks and Delayed Policy Updates

target network 없이 value estimation을 update할 경우 valatility가 증가하지만 fixed policy일 때 수렴 결과를 보여준다. 하지만 policy가 current value estimation으로 학습이 될 때 update가 된다면 발산할 수 있다. 

그래서 policy network는 value network update보다 느린 frequency로 update되어야한다. 

이 논문에서는 d번 이후 policy network와 target network를 update한다.

# Target Policy Smoothing Regularization

SARSA learning update를 흉내내어 regularization method를 제안한다. 이 방법은 비슷한 action은 비슷한 value를 가질 것이다라는 생각을 베이스로 만들어졌다. 작은 noise를 target policy에 넣은 후 mini-batch 단위로 평균을 취하는 방법을 이용한다.

$$ y = r + \gamma Q_{\theta'} (s', \pi_{\phi'}(s') +\epsilon)$$

$$\epsilon \sim \text{clip}(N(0,\sigma), -c, c)$$

# Algorithm

![algorithm][algo]
_Algorithm_

# Comment

DDPG 베이스에 그리 어려운 개념이 들어간 부분이 없어서 편하게 읽을 수 있는 논문이다. 현재까지 SAC를 가장 좋은 RL 논문이라고 생각을 하고 이 논문을 봤는 데 실험결과를 보면 SAC 비교할만한 결과를 보여주고 있다는 것에 놀랐다. 도메인에 따라 다르겠지만 SAC와 TD3를 비교하여 좋은 알고리즘을 적용해야할 것같다. 또한 SAC와 TD3를 합친 방법론이 논문으로 나와 실험 결과가 있을 것 같은데 한번 찾아보면 좋을 것 같다.

# Reference
1. [Fujimoto, Scott, Herke Hoof, and David Meger. "Addressing function approximation error in actor-critic methods." International Conference on Machine Learning. PMLR, 2018.][paper]

[paper]: http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf

[algo]: /assets/img/Single-agent/TD3/algo.png


---
title: 13. Actor-Critic
author: Jungi Lee
date: 2021-05-07 02:00:00 +0900
categories: [Reinforcement Learning, Single-Agent]
tags: [RL, Single-Agent]
math: true
mermaid: true
---

# 목차 

1. [Actor Critic](#actor-critic)  
1. [Algorithm](#algorithm)
1. [Reference](#reference)  

# Actor Critic

REINFORCEMENT 알고리즘에서 $$G_t$$ 대신 Q를 이용하는 방법이다. Q를 이용하기 때문에 actor와 critic 두 가지 네트워크를 이용한다.

$$\nabla_\theta J_\theta \simeq \int_\tau \underset{t=0}{\overset{\infty}{\sum}} \nabla_\theta \text{ln} P_\theta (a_t | s_t) G_t P_\theta(\tau) \, d \tau$$

$$P_\theta (\tau)$$는 아래 식으로 변환이 가능하다(bellman equation)

$$P_\theta (\tau) = P_\theta (s_{t+1}, a_{t+1}, \cdots | s_0, a_0 \cdots s_t, a_t) P_\theta (s_0, a_0, \cdots s_t, a_t)$$

$$P_\theta$$ 변환

$$ = \underset{t=0}{\overset{\infty}{\sum}} \int_{s_0,a_0 \cdots s_t, a_t} \nabla_\theta \text{ln} P_\theta (a_t | s_t) $$

$$\int_{s_{t+1},a_{t+1}, \cdots} G_t P_\theta (s_{t+1}, a_{t+1}, \cdots | s_t, a_t) \, ds_{t+1},a_{t+1} \cdots P_\theta(s_0,a_0, \cdots s_t, a_t) \, d s_0, a_0 \cdots s_t, a_t$$

위 식에서 가운데 적분 term은 Q function이다

$$Q(s_t a_t) = \int_{s_{t+1},a_{t+1}, \cdots} G_t P_\theta (s_{t+1}, a_{t+1}, \cdots | s_t, a_t) \, ds_{t+1},a_{t+1} \cdots $$

$$= \underset{t=0}{\overset{\infty}{\sum}} \int_{s_0,a_0 \cdots s_t, a_t} \nabla_\theta \text{ln}_\theta P_\theta(a_t |s_t) Q(s_t, a_t) P_\theta (s_0,a_0, \cdots s_t, a_t) ds_0,a_0 \cdots s_t,a_t$$

$$\int_x p(x,y) \,dx = p(y)$$ 라는 공식을 위식에 적용한다.

$$=\underset{t=0}{\overset{\infty}{\sum}} \int_{s_t,a_t} \nabla_\theta \text{ln} P_\theta(a_t | s_t) Q(s_t, a_t) P(s_t, a_t) \, ds_t, a_t$$

REINFORCEMENT의 경우 $$G_t$$가 있어서 에피소드가 다 끝나고 reword를 모아야하기 때문에 마지막에 1번 update를 하게된다.

Actor critic은 Q가 있어 approximation sampling으로 끝까지 가지 않아도 된다.

Q는 TD error를 이용하여 update한다.

# Algorithm

0. Initialize $$\theta, w$$

1. Actor update: $$\theta \gets \theta + \alpha \nabla_\theta \text{ln} P_\theta (a_t \vert s_t) Q_w(s_t \vert a_t) $$

2. Critic update: $$w \gets w - \beta \nabla_w (R_t + \gamma Q_w (s_{t+1}, a_{t+1}) - Q_w(s_t,a_t))^2$$

- $$R_t + \gamma Q_w(s_{t+1}, a_{t+1})$$는 상수 부분으로 상수 취급을 하지 않으면 TD target이 $$Q_w$$에 맞게 변화될 수 있다.

# Reference
1. [혁펜하임 유투브][혁펜하임 유투브]  

[혁펜하임 유투브]: https://www.youtube.com/watch?v=cvctS4xWSaU&list=PL_iJu012NOxehE8fdF9me4TLfbdv3ZW8g  

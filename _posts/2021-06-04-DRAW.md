---
title: (Gregor 2015 ICML) DRAW; A Recurrent Neural Network For Image Generation
author: Jungi Lee
date: 2021-06-04 00:09:00 +0900
categories: [Deep learning]
tags: [Deep learning]
math: true
mermaid: true
---
# 목차 
1. [Introduction](#introduction)  
1. [DRAW Architecture](#draw-architecture)  
1. [Read and Write operations](#read-and-write-operations)
1. [Reference](#reference)  

# Introduction
이 논문은 generative model 논문으로 VAE와 비슷한 느낌이다. 하지만 VAE와 다르게 RNN 구조가 추가된다. 왜 RNN 구조가 추가 되었나? 사람들이 그림을 그릴 때에서 아이디어를 가져왔다고 한다. 사람들이 그림을 그릴 때 전체를 한번에 그리는 것이 아닌 일부분에 집중을 하여 점차적으로 큰 그림을 그린다. 이러한 구조가 RNN처럼 time series로 볼 수 있다. 즉, one shot image 대신 image를 improve한다는 것이다.

P(C)가 joint distribution이라고 할 시 $$P(C) = P(A\vert B)P(B)$$로 표현할 수 있고 prior P(B)는 또 다른 distribution인 $$P(B \vert D)P(D)$$로 표현할 수 있다. 즉 time series 관점에서 볼 때 

$$P(C) = P(C_t|C_{t-1})P(C_{t-1}|C_{t-2}) \cdots P(C_1|C_0)P(C_0)$$

이 된다.

# DRAW Architecture

DRAW의 구조는 아래 사진과 같다.

![architecture][architecture]
_DRAW architecture_

Time series data를 다루기 위해 RNN 구조를 가지고 있으며 VAE와 비슷하게 encoder, sample, decoder를 포함하고 있다. 특이한 부분은 read, writer 부분으로 이 부분은 이후 section에서 다룰 것이다.

세부적인 공식은 아래와 같다.

![eq1][eq1]

DRAW의 loss function은 reconstruction error, latent distribution에 관한 loss 두가지가 있다.
Reconstruction loss는 처음 넣었던 image가 DRAW 통해 생성된 이미지와의 차이를 의미한다. 즉 Input image에서 특징을 잘 추출할 수 있는 latent vector를 통하여 재구성된 이미지와의 차이이다. Latent loss는 $$Q(Z_t|h^{\text{enc}}_t)$$가 $$P(Z_t)$$에 가깝게 되도록 KL divergence를 사용한 식이다. 이때 $$P(Z_t)$$는 평균이 0이고 분산이 1인 gaussian distribution이다. 전체 loss는 이 두 loss 더한 값이다. 이 부분은 VAE와 비슷한 부분으로 이해가 안가는 부분이 있다면 VAE를 공부하는 것이 좋을 것같다.

![eq2][eq2]
_Reconstruction Loss_

![eq3][eq3]

![eq4][eq4]
_Latent Loss_

# Read and Write Operations

여기서 Read란 전체 이미지에서 일부분(patch)를 읽어오는 것을 의미한다. 여기서 attention model이 사용되는 데 gaussian filter를 이용하여 구현하였다. 원본 이미지에서 attention model의 중심이 될 지점을 가져온 후 여기서 stride만큼 떨어진 곳에 gaussian filter를 적용한다.

아래 식을 통하여 중심 지점을 구하고 

![eq6][eq6]
_Attention parameter_

아래 식을 이용하여 각 gaussian filter의 평균을 계산한다.

![eq7][eq7]
_Gaussian Filter mean_

그리고 아래식을 이용하여 원본 이미지와 patch의 gaussian filter를 만든다. $$Z_x$$, $$Z_y$$는 Fx, Fy의 sum을 1로 만들기 위한 변수이다.


Attenntion parameter는 각각 (batch size, 1)의 크기를 가지고 Gaussian filter의 평균은 (batch_size, N, 1)의 크기를 가진다. Gaussian filter는 (N,A), (N,B)의 크기를 가진다. 여기서 N은 patch size, A, B는 이미지 크기를 의미한다.

Read operation은 원본 이미지와 Reconstruction image에 gaussian filter를 적용하여 patch로 만드는 attention model을 적용한 결과이고 Writer operation은 patch에서 원본 크기로 다시 복구하는 결과를 보여준다.

![read][read]
_Read_

![write][write]
_Write_

Read, writer를 반복적으로 실행하고 이전에 나왔던 출력과 hidden layer들을 이용하여 일부분을 관찰하여 복구하면서 결국 최종적으로 한 이미지를 복구하게 된다. 

아래는 내가 구현한 코드의 결과이다. 점차적으로 이미지가 그려지는 것을 확인할 수 있다. 

![result][result]

# Reference
1. [Gregor, Karol, et al. "Draw: A recurrent neural network for image generation." International Conference on Machine Learning. PMLR, 2015.][paper]


[paper]: http://proceedings.mlr.press/v37/gregor15.pdf 

[architecture]: /assets/img/DNN/DRAW/architecture.png
[eq1]: /assets/img/DNN/DRAW/eq1.png
[eq2]: /assets/img/DNN/DRAW/eq2.png
[eq3]: /assets/img/DNN/DRAW/eq3.png
[eq4]: /assets/img/DNN/DRAW/eq4.png
[eq5]: /assets/img/DNN/DRAW/eq5.png
[eq6]: /assets/img/DNN/DRAW/eq6.png
[eq7]: /assets/img/DNN/DRAW/eq7.png
[read]: /assets/img/DNN/DRAW/read.png
[write]: /assets/img/DNN/DRAW/write.png
[result]: /assets/img/DNN/DRAW/result.gif

---
title: (Lasse 2018 ICML) Impala; Scalable distributed deep-rl with importance weighted actor-learner architectures
author: Jungi Lee
date: 2021-09-20 01:00:00 +0900
categories: [Reinforcement Learning, Distributed RL]
tags: [RL, Distributed RL]
math: true
mermaid: true
---
# 목차 
1. [Introduction](#introduction)  
1. [IMPALA](#impala)  
1. [V-trace](#v-trace)  
1. [Reference](#reference)  

# Introduction

강화학습에는 빠른 학습을 위해서 A3C와 같은 off-policy based distributed learning 방식을 사용한다. A3C는 worker들 간의 gradient을 공유하여 central parameter server를 update하는 방식이다. IMPALA는 experience의 trajectory를 공유를 하여 centralised learner를 학습시킨다. 

# IMPALA 

기존 learner policy의 update를 여러 개의 actor를 통해서 학습한다면 policy-lag이라는 actor와 learner 사이의 차이가 발생한다. 이 논문에서는 V-trace라는 방법을 제안하여 이 차이를 줄여 높은 학습 결과와 data efficiency를 얻을 수 있다고 주장한다. 

# V-trace

이 논문의 핵심 파트이다. V-trace는 A2C와 동일하게 policy와 value function을 학습시킨다. V-trace의 value function의 target은 다음과 같다.


$$v_s \overset{\text{def}}{=} V(x_s) + \underset{t=s}{\overset{s+n-1}{\sum}} \gamma^{t-s} (\underset{i=s}{\overset{t-1}{\prod}}c_i)\delta_t V$$

where $$\delta_t V \overset{\text{def}}{=} \rho_t + \gamma V(x_{t+1}) - V(x_t))$$ is temporal difference for V, and $$\rho \overset{\text{def}}{=} \text{min}(\bar{\rho}, \frac{\pi(a_t \vert x_t)}{\mu (a_t \vert x_t)})$$, and $$c_i \overset{\text{def}}{=} \text{min}(\bar{c}, \frac{\pi(a_i \vert x_i)}{\mu(a_i \vert x_i)})$$

기존 A2C의 target function과는 다르다. 하지만 $$c_i$$ =1, $$\rho_t=1$$=1 일시 target function은 A2C와 동일하게 된다.

$$\rho_t$$와 $$c_i$$의 역할은 다르다. $$\rho_t$$는 temporal difference에 나타나며 이 update rule의 fixed point를 정의한다. $$\bar{\rho}$$가 infinite라면 value function이 target policy의 value function 된다. 만약 그렇지 않다면 $$\mu$$와 $$\pi$$의 value function 그 사이 어딘가가 될 것이다. $$c_i$$는 Retrace의 trace cutting coefficients와 비슷한 역할이다. 각 time t에서 value function이 얼마나 다른 지를 측정하는 값으로 $$\pi$$와 $$\mu$$가 다를 수록 이 값들의 product의 variance는 커진다. $$\rho$$값을 1, 10, 100로 설정하여 테스트하였으며 1이 가장 좋은 결과를 보여주었다고 하며 $$c_i$$보다 $$\rho$$에 큰 영향이 있다고 설명한다. $$\bar{\rho}$$는 value function convergence에 영향이 있으며 $$\bar{c}$$는 converge 속도에 영향이 있다.

# Comments
네트워크 구조와 실험 결과는 생략하였다. 이 부분은 논문을 통해 더 자세히 보는 것이 좋을 것으로 생각된다. 해당 수식이 converge함과 coefficient의 subjection은 제시가 되어있지만 V-trace구조가 어떤 수식에서 유도가 되었는 지에대해서 설명이 없어서 아쉬운 부분이 있다.(Retrace에서 가져왔는 거 같아서 Retrace를 찾아볼 예정이다.) 각 thread(worker, actor)들의 trajectory를 활용하기 위해서 off-policy 알고리즘에서 신경 쓴 것을 확인할 수 있다. 

# Reference
1. [Espeholt, Lasse, et al. "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures." International Conference on Machine Learning. PMLR, 2018.][paper]


[paper]: https://arxiv.org/pdf/1802.01561.pdf 



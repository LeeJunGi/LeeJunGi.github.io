---
title: (Haarnoja 2018 ICML) Soft Actor-Critic; Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor
author: Jungi Lee
date: 2021-06-26 01:50:00 +0900
categories: [Reinforcement Learning, Single-Agent]
tags: [RL, Single-Agent]
math: true
mermaid: true
---
# 목차 
1. [Introduction](#introduction)  
1. [Maximum Entropy](#maximum-entropy)  
1. [Derivation of Soft Policy Iteration](#derivation-of-soft-policy-iteration)  
1. [Soft Actor-Critic](#soft-actor-critic)  
1. [Algorithm](#algorithm)  
1. [Enforcing Action Bounds](#enforcing-action-bounds)  
1. [Reference](#reference)  

# Introduction

현실에서 강화학습을 적용하기 어려운 이유는 두 가지가 있다. 첫번째는 task 최적화를 위해서 많은 sample 관측이 필요하다는 것이다. 두번째는 좋은 결과를 위해서 hyperparameter의 조정이 어렵다는 것이다. 한가지 sample efficiency가 떨어지는 이유는 on-policy 때문이다. 그래서 TRPO나 PPO와 같은 논문에서는 off-policy 방법은 제안한다. 이 논문에서는 maximum entropy RL을 통하여 exploration을 잘하고 좀 더 robust한 결과를 얻을 수 있도록하였다. 

# Maximum Entropy

Maximum entropy 관련해서는 [TAC][https://leejungi.github.io/posts/tac/]에서 proposition 2를 보면 좀 더 이해하기 편할 것같다. 간단하게 얘기하면 무질서할 수록 entropy가 높아진다. 즉 확률이 평등해진다는 의미로 entropy를 maximize하는 의미는 exploration을 더하도록 권장한다는 의미이다. 


# Derivation of Soft Policy Iteration

Soft Actor-Critic(SAC)는 maximum entropy variant of the policy iteration method로부터 가져올 수 있다. 
SAC의 목적함수는 아래이다.

$$J(\pi) = \underset{t=0}{\overset{T}{\sum}} E_{(s_t, a_t) \sim \rho_\pi} [r(s_t,a_t) + \alpha H(\pi(\cdot \vert s_t))]$$

Q function은 시간이 지남에 따라 update가 될 것이고 Bellman backup opeartor $$\Gamma^\pi$$에 의해 변경될 것이다.

$$\Gamma^\pi Q(s_t, a_t) \triangleq r(s_t, a_t) +\gamma E_{s_{t+1} \sim p} [V(s_{t+1})]$$

where $$V(s_t) = E_{a_t \sim \pi} [Q(s_t, a_t) - \text{log} \pi(a_t  \vert s_t)]$$

그리고 새로운 policy는 exponential Q function에 가까워지도록 update 될것이다. 그리고 이를 위해서 KL divergence를 사용한다.

$$\pi_{\text{new}} = \underset{\pi' \in \Pi}{\text{argmin}} D_{\text{KL}}( \pi ' (\cdot \vert s_t) \vert\vert \frac{\text{exp}(Q^{\pi_{\text{old}}}(s_t, \cdot))}{Z^{\pi_{\text{old}}}(s_t)})$$

Z는 distribution을 normalize하는 기능이고 intractable하지만 gradient에 영향이 없기 때문에 무시한다.

# Soft Actor-Critic

Q와 V function은 TD target을 이용하여 update한다.

$$J_V(\psi) = E_{s_t \sim D} [\frac{1}{2} ( V_\psi(s_t) - E_{a_t \sim \pi_\phi}[Q_\theta(s_t,a_t) - \text{log} \pi_\phi(a_t \vert s_t)])^2]$$

$$J_Q(\theta) = E_{(s_t,a_t) \sim D} [\frac{1}{2} (Q_\theta(s_t, a_t) - \hat{Q} (s_t, a_t)) ^2]$$

where $$\hat{Q} (s_t, a_t) = r(s_t, a_t) + \gamma E_{s_{t+1} \sim p} [V_{\bar{\psi}}(s_{t+1})]$$

V를 이용하여 Q function을 approximation하여 network를 줄일 수 있으나 따로 Q와 V function을 두면 training이 안정적으로 된다는 논문 결과가 있어서 SAC는 따로 두었다.

policy update는 이전에 있던 KL divergence 식을 이용하여 update 된다.

$$J_{\pi}(\phi) = E_{s_t \sim D, \epsilon_t \sim N}[\text{log} \pi_\phi (f_\phi(\epsilon_t;s_t)\vert s_t) - Q_\theta (s_t, f_\phi(\epsilon_t ; s_t))]$$

policy 식을 update하는 방법을 여러가지가 있다. 대표적으로 likelihood ratio gradient estimator라는 방법이 있다. 이 방법은 backpropagate를 하지 않아도된다. 하지만 이 논문에서는 Q function이 NN으로 이루어져있고 미분 가능하기 때문에 reparameterization trick을 이용하여 편한 방법을 이용하여 update한다. 이 방법은 variance가 더 낮다는 장점이 있다.

# Algorithm

![algorithm][algo]
_Algorithm_


# Enforcing Action Bounds

원래 unbound action distribution을 사용하지만 실제로 사용하기 위해서는 bound된 action distribution을 사용해야한다. 그래서 이 논문에서는 tanh activation function을 이용한다. 그리고 likelihood를 계산하기 위해 variables formula를 바꾼다.

$$\pi(a \vert s) = \mu (u \vert s) \vert \text{det} (\frac{da}{du}) \vert ^{-1}$$

$$\text{log} \pi (a \vert s) = \text{log} \mu (u \vert s) - \underset{i=1}{\overset{D}{\sum}} \text{log} (1 - \text{tanh}^2 (u_i))$$



# Reference
1. [Haarnoja, Tuomas, et al. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." International Conference on Machine Learning. PMLR, 2018.][paper]

[paper]: https://arxiv.org/pdf/1801.01290.pdf

[algo]: /assets/img/Single-agent/SAC/algo.png


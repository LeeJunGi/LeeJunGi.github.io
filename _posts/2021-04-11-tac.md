---
title: (Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning
author: Jungi Lee
date: 2021-05-11 02:10:00 +0900
categories: [Reinforcement Learning, Multi-Agent]
tags: [RL, Multi-Agent]
math: true
mermaid: true
---

# 목차 
1. [Problem definition](#problem-definition)  
1. [Background](#background)  
	- [Proposition 1](#proposition-1)  
	- [Proposition 2](#proposition-2)  
1. [Bandit with Maximum Tsallis Entropy](#bandit-with-maximum-tsallis-entropy)  
	- [Proposition 3](#proposition-3)  
1. [Reference](#reference)  

# Problem definition

이전에 RL의 exploration을 더 잘하기 위해서 entropy 개념을 도입하여 최적화하였다. 그 대표적인 entropy가 Shannon-Gibbs(SG) entropy이다. 역설적으로 exploration을 더 잘하게 만들어서 exploit이 감소하여 performance가 감소하는 문제도 있다. 이를 해결하기 위해 entropy decaying과 같은 방법을 통해 해결하였다. 다른 방법으로는 sparse Tsallis(ST) entropy를 사용하는 방법이있다. ST entropy는 exploration을 더 적게하여 suboptimal policy를 찾을 수 있다는 문제가 있다. 그래서 해당 논문에서는 Tsallis MDP문제로 정의하여 다양한 entropy를 cover할 수 있도록 제안한다.  

# Background  

## Shannon-Gibbs entropy  
$$S = - \overset{n}{\underset{i=1}{\sum}} p_i \operatorname{log}p_i$$  
이 entropy를 maximize하는 의미는 $$p_i$$를 모든 state에 대해서 같은 확률로 만든다는 의미이다.  

$$- \overset{n}{\underset{i=1}{\sum}} p_i \operatorname{log}p_i \leq - \overset{n}{\underset{i=1}{\sum}} \frac{1}{n} \operatorname{log}{\frac{1}{n}}$$  

## q-Exponential, q-Logarithm, and Tsallis Entropy  

$$ \operatorname{exp_q}(x) \triangleq \begin{cases} \operatorname{exp}(x) &\text{if q = 1}\\ [1+(q-1)x]^{\frac{1}{q-1}}_+ &\text{if q} \neq \text{1}\end{cases}$$  
$$ \operatorname{ln}_q(x) \triangleq \begin{cases} \operatorname{log}(x) &\text{if q = 1 and x > 0}\\ \frac{x^{q-1}-1}{q-1} &\text{if q} \neq \text{1 and x > 0}\end{cases} $$  
where $$[x]_+ = \operatorname{max}(x,0) $$ and q is a real number

위 공식은 q-exponential, q-logarithm 공식이다. $$\operatorname{ln}_q(x)$$는 x에대해 단조 증가함수이다. q가 1일 경우 log(x)가 되며 q가 2일 경우 선형 함수가 된다. 하지만 q>2일 경우 $$d\operatorname{ln}_q (x)/dx$$는 증가, q<2일 경우 $$d\operatorname{ln}_q (x)/dx$$는 감소 함수가 된다.

Tsallis entorpy는 $$\operatorname{ln}_q(x)$$로 표현할 수 있다.  

$$S_q(P) \triangleq \underset{X \sim P}{E} [-\operatorname{ln}_q(P(X))]$$  

q가 1일 경우 SG entropy가 되고 q가 2일 경우 ST entropy가 된다. q가 무한대일 경우 entropy는 0이 된다. q>0일 경우 Tsallis entropy는 concave function이 된다.  

### Proposition 1

<details>
<summary> Concave 증명</summary>
<div markdown="1">


Assume that X is a finite space. Let P is a probability distribution over X. if q>0, then, $$S_q(P)$$ is concave with respect to P.

proof. $$f(x) = -x \operatorname{ln}_q(x)$$이라고 정의(x>0). 이를 두번 미분.

1번 미분  
$$\frac{df(x)}{dx} = -\operatorname{ln}_q(x) -x\operatorname{ln}_q^{'}(x)$$ 

2번 미분  
$$\frac{d^2f(x)}{dx^2} = -\operatorname{ln}_q^{'}(x) -\operatorname{ln}_q^{'}(x) -\operatorname{ln}_q^{''}(x) = -2 \frac{(q-1)x^{q-2}}{q-1} - x\frac{(q-1)(q-2)x^{q-3}}{q-1}$$
$$= -2x^{q-2}-x(q-2)x^{q-3} = -qx^{q-2}$$

$$\frac{d^2f(x)}{dx^2} = -qx^{q-2} <0(x>0, q>0)$$

-q는 항상 음수이고 x의 지수함수는 항상 양수이기 때문에 2번 미분한 값은 항상 음수로 나온다. 이 결과를 이용하여 다음과 같은 특성이 만족하는 것을 보일 수 있다.

$$\lambda_1, \lambda_2 \geq 0$$, $$\lambda_1+\lambda_2 =1$$일 경우 probability $$P_1, P_2$$에 대해서 아래의 식을 만족한다.(중간값 정리)  

$$S_q(\lambda_1P_1 + \lambda_2P_2) = \underset{x}{\sum}-(\lambda_1P_1(x) + \lambda_2P_2(x))\operatorname{ln}_q(\lambda_1P_1(x)+\lambda_2P_2(x))$$
$$\ge \underset{x}{\sum} -\lambda_1P_1(x)\operatorname{ln}_q(P_1(x)) - \lambda_2P_2(x) \operatorname{ln}_q(P_2(x))$$
$$=\lambda_1S_q(P_1) + \lambda_2S_q(P_2)$$

*위 식의 부등식(>)은 논문 부등식(<) 반대로 되어있다. 내가 생각하였을 때 중간값 정리를 적용하면 논문과 다르게 부등식이 반대로 되어야한다고 생각된다. 또한 뒤쪽에서 위 식을 사용하는 부분에서는 부등식이 반대로 되어 있기 때문에 > 부등식을 사용하는 것이 맞다고 생각한다.

그러므로 $$S_q(P)$$는 P에 대해서 concave하다.

</div>
</details>

### Proposition 2
<details>
<summary> Entropy가 최대가되는 P 값 증명</summary>
<div markdown="1">
Assume that X is a finite space. Then, S_q(P) is maximized when P is a uniform distribution. 
i.e., P= 1|X| where |x| is the number of elements in X. 

proof. KKT condition을 이용하여 optimization problem 해결

기본적으로 discrete uniform distribution일 때 entropy가 최대값을 가진다는 것을 알고 있다. 이 논문에서는 다시 한번 증명을 하고 있고 이 내용을 후반 부 증명에서 사용했다.

$$\underset{P \in \triangle}{\operatorname{max}} S_q(P)$$

where, $$\triangle = \{P \mid P(x) \geq 0, \underset{x}{\sum} P(x) =1\}$$ is a probability simplex.

KKT condition: 

$$\forall x \in X, \frac{\partial(S_q(\pi)-\underset{x}{\sum}\lambda^*(x)P(x) - \mu^*(1-\underset{x}{\sum}P(X)))}{\partial P(x)} \bracevert_{P(x)=P^*(x)}$$
$$ = -\operatorname{ln}_q(P^*(x))-(P^*(x))^{q-1}-\lambda^*(x) + \mu^*$$
$$ =-q \operatorname{ln}_q(P^*(x)) -1 -\lambda^*(x) + \mu^* =0$$  
$$\forall x \in X, 0 = 1 -\underset{x}{\sum}P^*(x), P^*(x) \geq 0 $$  
$$\forall x \in X, \lambda^*(x) \leq 0$$  
$$\forall x \in X, \lambda^*(x)P^*(x) =0$$

where $$\lambda^*$$ and $$\mu^*$$ are the Lagrangian multipliers for constraints in $$\triangle$$  

세부 증명 1.  
$$\frac {\partial S_q(\pi)} {\partial P(x)} = \frac{P(x)^{q-1}-1}{q-1} + P(x) \frac{(q-1)P(x)^{q-2}}{q-1}$$  
$$=\frac{P(x)^{q-1}-1}{q-1} + \frac{(q-1)P(x)^{q-1}}{q-1}$$
$$=\frac{qP(x)^{q-1}-1}{q-1}$$
$$=\frac{qP(x)^{q-1} -q -1 +q}{q-1}$$
$$=q\operatorname{ln}_qP(x)-1$$

세부 증명 2. 첫번째 KKT condition을 이용하여 $$P^*(x)$$에 대해서 정리  
$$P^*(x) \ge 0$$이므로 $$\lambda^*(x) =0$$이 된다.  

$$-q\operatorname{ln}_q(P^*(x)) -1 + \mu^*=0$$  
$$\operatorname{ln}_q(P^*(x)) = \frac{-1+\mu^*}{q}$$  
$$\frac{P^*(x)^{q-1}-1}{q-1} = \frac{-1 + \mu^*}{q}$$  
$$P^*(x)^{q-1}-1 = (q-1) \frac{-1 + \mu^*}{q}$$  
$$P^*(x) = (1+(q-1)\frac{-1 + \mu^*}{q})^{\frac{1}{q-1}}=\operatorname{exp}_q(\frac{\mu^* -1}{q})$$

P는 $$\mu$$와 q에 의해서만 결정된다.(constant probability mass) x에 대한 함수가 아니므로 $$P^*(x)=1/\vert S \vert$$ where $$S=\{x \mid P^*(x) \ge0\}$$이다.(P가 모든 확률에 대한 uniform이 아닌 일부 확률에 대해서만 동일한 확률을 가질 수 있다. 물론 $$P^*(x) \ge 0$$이라고 condition이 있으므로 이미 uniform이라고 해도 될 것같다.)

Optimal value는 $$S_q(P^*)=-\operatorname{ln}_q(1/ \vert S \vert)$$이다. $$-\operatorname{ln}_q(x)$$는 단조 증가함수이므로 $$\mid S \mid$$는 최대가 되어야한다. S=X일 때 S가 최대가 되므로 $$P^*(x) = 1/\vert X \vert $$이다.  





</div>
</details>




# Bandit with Maximum Tsallis Entropy  

MDP에 적용하기 전에 간단한 예제인 multi-armed bandit(MAB) 문제에 적용을 한다. MAB 문제에서 정의한 특성들이 MDP 문제에도 적용을 할 수 있다.  

$$\underset{\pi \in \triangle}{\operatorname {max}} [\underset{a \sim \pi}{E}[R] + \alpha S_q(\pi)]$$

MAB의 Tsallis entropy maximization은 위 식으로 정의된다.

$$\pi_q^*(a) = \underset{a}{\sum} \operatorname{exp}_q (r(a)/q- \psi(r/q))$$  

Tsallis entroy의 stochastic optimal policy는 위 식으로 정의된다. $$\psi$$는 q-potential function으로 q-expoential, q-logarithm과 같은 q가 변수로 사용되는 function이다.  


$$ \underset{a \sim \pi^*}{E}[R] + \alpha S^*_q(\pi)=(q-1) \underset{a}{\sum}\frac{r(a)}{q} \operatorname{exp}_q (\frac{r(a)}{q} - \psi_q(\frac{r}{q})) + \psi_q (\frac{r}{q})$$

$$\pi^*_q$$를 이용한 optimal value는 위 식으로 정의된다. q가 1일 경우 optimal policy는 softmax distribution이 된다. q가 2일 경우 ST entropy를 이용한 optimal policy가 된다. $$\pi^*_1$$과 다르게 $$\pi^*_2$$는 0 probability를 만들 수 있다.($$r(a) < 2\psi_2(r/2) -2$$일 경우) q가 무한대일 경우 기본적인 MAB problem이 된다. q가 1,2, 무한대 일경우 $$\psi_q$$구하기 쉬웠지만 이외의 경우는 어렵다. 다른 논문에서 이를 first order tayler expansion으로 구하는 방법을 소개했지만 이 논문에서는 다른 방법을 사용한다.(section 6에서 다뤄진다. network를 이용하여 update하는 방법을 사용)  

### Proposition 3
<details>
<summary> Optimal solution of MAB with maximum Tsallis entropy</summary>
<div markdown="1">

The optimal solution of 

$$\underset{\pi \in \triangle}{\operatorname{max}} [ \underset{a \sim \pi}{E} [R] + S_q(\pi)]$$

is

$$\pi^*_q(a) = \operatorname{exp}_q(\frac{r(a)}{q} - \psi_q(\frac{r}{q}))$$

where the q-potential function $$\psi_q$$ is a functional define on {A,r}

$$\psi$$는 아래의 공식의 condition에 의해서 결정된다.

$$\underset{a}{\sum}\pi^*_q(a) = \underset{a}{\sum} \operatorname{exp}_q (\frac{r(a)}{q} - \psi (\frac{r}{q})) =1 $$

$$\psi^*_q$$를 이용해서 optimal value는 아래와 같이 정의할 수 있다.

$$\underset{\pi \in \triangle}{\operatorname{max}} [ \underset{a \sim \pi}{E} [R] + S_q(\pi)] = (q-1) \underset{a}{\sum} \frac{r(a)}{q} \operatorname{exp}_q(\frac{r(a)}{q} - \psi_q \frac{r}{q}) + \psi_q(\frac{r}{q})$$

proof. $$\operatorname{exp}_q \in [0, \infty)$$는 continous 단조 함수 이기 때문에 $$\underset{a}{\sum} \operatorname{exp}_q(\frac{r}{q} - \xi)$$는 0으로 수렴하고 $$\xi$$가 +,- 무한대일 경우 무한대로 수렴한다. 그러므로 $$\underset{a}{\sum} \operatorname{exp}_q(\frac{r}{q} - \xi^*)=1$$을 만족하는 $$\xi^*$$가 존재한다. 그러므로 $$\psi_q(r/q) = \xi^*$$라고 할 수 있다.

나머지는 KKT condition을 고려하여 구한다.
KKT conditon:

$$\forall i, 1-\underset{a}{\sum} \pi^*_q(a) = 0, \pi^*_q(a) \geq 0$$  
$$\forall i, \lambda^*(a) \leq 0$$  
$$\forall i, \lambda^*(a)\pi^* =0$$  
$$\forall i, r(a)-\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (\pi^*_q(a))^{q-1}+\lambda^*(a)=0$$

마지막 condition은 아래 식으로부터 나온다.
$$\frac{\partial}{\partial \pi}(\underset{a \sim \pi}{E}[R] + S_q(\pi) + \mu^*(1-\underset{a}{\sum} \pi(a))+\lambda^*(a)\pi(a))$$

마지막 condition은 아래와 같이 변환이 가능하다.
$$0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (\pi^*_q(a))^{q-1} + \lambda^*(a)$$  
$$0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\frac{(\pi^*_q(a))^{q-1}}{q-1} + \lambda^*(a)$$  
$$0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\frac{(\pi^*_q(a))^{q-1}+1-1}{q-1} + \lambda^*(a)$$  
$$0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\frac{(\pi^*_q(a))^{q-1}-1}{q-1} -1 + \lambda^*(a)$$  
$$0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\operatorname{ln}_q(\pi^*(a)) -1 + \lambda^*(a)$$  
$$0=r(a) -\mu^* - q\operatorname{ln}_q(\pi^*_q(a)) -1 + \lambda^*(a)$$  

$$\pi^*_q(a) \ge 0$$이므로 $$\lambda^*(a) =0$$이 된다. 그러므로 위 공식은 아래와 같이 표현된다.

$$0=r(a) -\mu^* - q\operatorname{ln}_q(\pi^*_q(a)) -1 $$  
$$q\operatorname{ln}_q(\pi^*_q(a))=r(a) -\mu^*  -1 $$  
$$\operatorname{ln}_q(\pi^*_q(a))=\frac{r(a)}{q} -\frac{\mu^*  +1}{q} $$  
$$\pi^*_q(a) = \operatorname{exp}_q(\frac{r(a)}{q} - \frac{\mu^*+1}{q})$$

이제 $$\mu^*$$를 구해야한다. 이 값은 $$\underset{a}{\sum}\pi^*_q(a)$$를 이용해 구할 수 있다.
$$\underset{a}{\sum}\pi^*_q(a) = \underset{a}{\sum} \operatorname{exp}_q (\frac{r(a)}{q} - \frac{\mu^*+1}{q})=1$$

이전에 $$\underset{a}{\sum} \operatorname{exp}_q (\frac{r(a)}{q} - \psi_q (\frac{r}{q}))=1$$로 표현하였다. 위 두 식이 동일하다고 하면

$$\psi(\frac{r}{q}) = \frac{\mu^*+1}{q}$$

이 된다. 이 식은 $$\mu^*$$에 대해서 풀면 해당 값을 얻을 수 있다.

$$\mu^* = q\psi_q(\frac{r}{q})-1$$

$$\pi^*_q(a) \ge 0$$를 만족해야한다.(KKT condition)

$$1+(q-1)(\frac{r(a)}{q} - \psi(\frac{r}{q})) \ge 0$$ 

이므로 $$\pi^*_q(a) \ge 0$$를 만족한다.(위 식은 $$\pi^*_q(a)$$의 definition에 의한 식이다.)

- $$\mu^*$$가 이전 공식에서 비교해서 가져오는 것이 아닌 논문 저자의 수식 표현으로 정의할 때어디서 이 표현 수식의 idea를 가져왔는 지 과정이 궁금하다.

Optimal value problem은 아래의 수식으로 표현 가능하다.

$$\underset{a \sim \pi^*_q}{E}[R] + S_q(\pi^*_q)$$  
$$=\underset{a}{\sum}r(a)\pi^*_q(a) - \underset{a}{\sum}[\frac{r(a)}{q} - \psi(\frac{r}{q})]\pi^*_q(a)$$  
$$=\underset{a}{\sum}[r(a)\pi^*_q(a) - \frac{r(a)}{q}\pi^*_q(a) + \underset{a}{\sum}\psi_q(\frac{r}{q})\pi^*_q(a)]$$  
$$=\underset{a}{\sum} \frac{qr(a)-r(a)}{q} \pi^*_q(a) + \psi_q(\frac{r}{q})\underset{a}{\sum}\pi^*_q(a)$$  
$$=(q-1)\underset{a}{\sum}\frac{r(a)}{q}\pi^*_q(a) + \psi_q(\frac{r}{q})$$  
$$=(q-1)\underset{a}{\sum}\frac{r(a)}{q}\operatorname{exp}_q(\frac{r(a)}{q} - \psi_q(\frac{r}{q})) + \psi_q(\frac{r}{q})$$  

</div>
</details>

### Proposition 4
<details>
<summary> q에 따른 policy, q-potential 값 구하기</summary>
<div markdown="1">

When q=1 and q=2, $$\psi_1, \pi^*_1$$ and $$\psi_2, \pi^*_2$$의 연산 결과는 아래와 같다.

$$\pi^*_1 = \operatorname{exp}(r(a)-\psi_1(r))$$

$$\psi_1(r) = \operatorname{log}\underset{a}{\sum}\operatorname{exp}(r(a))$$

$$\pi^*_2 = [1+ (\frac{r(a)}{2} - \psi_2(\frac{r}{2}))]_+$$

$$\psi_1(\frac{r}{2}) = 1+ \frac{\underset{a \in S}{\sum}r(a)/2 -1}{\vert S \vert}$$

proof. 

이전 공식 $$\pi^*_q(a) = \underset{a}{\sum} \operatorname{exp}(\frac{r(a)}{q} - \frac{\mu^*+1}{q})$$를 이용

q=1일 경우

$$\pi^*_1$$는 $$\operatorname{exp}(r(a) -\psi_1(r))$$이 된다.(이전의 $$\pi^*_q$$의 공식을 이용)

$$\underset{a}{\sum} \operatorname{exp}(r(a) - \mu^*-1)=1$$

$$\underset{a}{\sum} \operatorname{exp}(r(a)) \operatorname{exp}(- \mu^*-1)=1$$

$$\underset{a}{\sum} \operatorname{exp}(r(a)) = \operatorname{exp}(\mu^*+1)$$

$$\operatorname{log}(\underset{a}{\sum} \operatorname{exp}(r(a))) = \mu^* +1$$

$$\mu^* +1 = \psi_1(r(a))$$이므로

$$\psi_1(r) = \operatorname{log} \underset{a}{\sum} \operatorname{exp}(r(a))$$

q=2일 경우 

$$\pi^*_2$$는 $$\operatorname{exp}(\frac{r(a)}{2} -\psi_2(\frac{r}{2}))$$이 된다.

$$\underset{a \in S}{\sum} \operatorname{exp}_2(\frac{r(a)}{2} - \frac{\mu^*+1}{2}) = \underset{a \in S}{\sum}(1+\frac{r(a)}{2} - \frac{\mu^*+1}{2})=1$$

$$\underset{a \in S}{\sum} (1 + \frac{r(a)}{2}) = \underset{a \in S}{\sum} (\frac{\mu^*+1}{2}) +1$$

$$\underset{a \in S}{\sum} (1 + \frac{r(a)}{2}) -1 = \vert S \vert (\frac{\mu^*+1}{2})$$

$$ \frac{\underset{a \in S}{\sum} (1 + \frac{r(a)}{2})-1}{\vert S \vert} = \frac{\mu^* +1}{2}$$

$$ \frac{\vert S \vert + \underset{a \in S}{\sum} (\frac{r(a)}{2})-1}{\vert S \vert} = \frac{\mu^* +1}{2}$$

$$\frac{\mu^* +1}{2} = \psi_2(\frac{r}{2})$$이므로

$$\psi_2(\frac{r}{2})=1+ \frac{\underset{a \in S}{\sum}r(a)/2 -1}{\vert S \vert}$$



</div>
</details>

# Reference

1. [Lee, Kyungjae, et al. "Tsallis reinforcement learning: A unified framework for maximum entropy reinforcement learning." arXiv preprint arXiv:1902.00137 (2019).] [TAC]

[TAC]: https://arxiv.org/pdf/1902.00137.pdf  

### Propo
<details>
<summary> Optimal solution of MAB with maximum Tsallis entropy</summary>
<div markdown="1">


</div>
</details>



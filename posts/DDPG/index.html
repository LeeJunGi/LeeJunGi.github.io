<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning" /><meta name="author" content="Jungi Lee" /><meta property="og:locale" content="en_US" /><meta name="description" content="목차 Introduction Contribution Stochastic vs Deterministic policy Deterministic policy Additional Algorithm Reference" /><meta property="og:description" content="목차 Introduction Contribution Stochastic vs Deterministic policy Deterministic policy Additional Algorithm Reference" /><link rel="canonical" href="https://leejungi.github.io/posts/DDPG/" /><meta property="og:url" content="https://leejungi.github.io/posts/DDPG/" /><meta property="og:site_name" content="JG_blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-06-20T23:50:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Jungi Lee" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Jungi Lee"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://leejungi.github.io/posts/DDPG/"},"description":"목차 Introduction Contribution Stochastic vs Deterministic policy Deterministic policy Additional Algorithm Reference","url":"https://leejungi.github.io/posts/DDPG/","@type":"BlogPosting","headline":"(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning","dateModified":"2021-06-20T23:50:00+09:00","datePublished":"2021-06-20T23:50:00+09:00","@context":"https://schema.org"}</script><title>(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning | JG_blog</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/carrot.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JG_blog</a></div><div class="site-subtitle font-italic">RL researcher and developer</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/leejungi" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ganbbang12','naver.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Jun 20, 2021, 11:50 PM +0900" > Jun 20, 2021 <i class="unloaded">2021-06-20T23:50:00+09:00</i> </span> by <span class="author"> Jungi Lee </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1198 words">6 min</span></div></div><div class="post-content"><h1 id="목차">목차</h1><ol><li><a href="#introduction">Introduction</a><li><a href="#contribution">Contribution</a><li><a href="#stochastic-vs-deterministic-policy">Stochastic vs Deterministic policy</a><li><a href="#deterministic-policy">Deterministic policy</a><li><a href="#additional">Additional</a><li><a href="#algorithm">Algorithm</a><li><a href="#reference">Reference</a></ol><h1 id="introduction">Introduction</h1><p>이 논문은 DQN을 응용한 논문이다. DQN은 low-dimensional action space에 관한 문제들만 해결이 가능하다. 즉 사람의 팔과 같은 7 degree of freedom을 가지는 문제의 경우 높은 차원을 가지고 있기 때문에 해결하기가 어렵다. 그리고 Continuous space에서 사용하기 위해 넓은 범위를 discretization을 하여 사용하였다. 하지만 discretization을 한다면 정보 손실이 생길 것이다. 이전의 actor-critic algorithm과 다르게 stochastic이 아닌 determinstic을 이용하여 DQN의 actor critic version이라고 생각할 수 있을 것같다.</p><h1 id="contribution">Contribution</h1><p>이 논문의 contribution은 아래와 같다.</p><ol><li>Off-policy: 이전 논문들은 importance sampling을 통하여 off-policy로 만들었지만 이 논문은 그렇지 않다. stochastic이 아닌 deterministic policy를 사용함으로써 큰 replay buffer를 사용할 수 있고 reset을 할 필요가 없다.<li>target q network: target network를 따로 두어 update하는 동안 안정적으로 만든다.<li>Soft update: target network를 완전히 update하는 것이 아닌 \(\tau\)만큼만 업데이트를 한다. soft update는 target network가 천천히 안정적으로 update되게 만든다.</ol><h1 id="stochastic-vs-deterministic-policy">Stochastic vs Deterministic policy</h1><p>이전 stochastic policy는 \(\mu\), \(\sigma\)를 가진 distribution에서 sample을 뽑는다. 하지만 deterministic policy는 어떤 distribution에서 sample을 뽑는 것이 아닌 한 가지 값을 선택하는 것이다.</p><p>stochastic policy의 경우 mean, variance를 학습하고 이 mean, variance의 distribution에서 sampling을 통하여 action을 선택한다면 deterministic policy는 output이 바로 action이 된다. 즉 deterministic policy는 \(\delta\)-function이다.</p><h1 id="deterministic-policy">Deterministic policy</h1><p>Deep Deterministic Policy Gradient(DDPG) 또한 Policy gradient 방식이므로 목적함수가 동일하다.</p>\[J_\theta = E[G_0] = \int_{s_0:a_\infty} G_0 P(s_0:a_\infty) \, d{s_0:a_\infty}\] \[= \int_{s_0:a_\infty} G_0 P(a_0:a_\infty \vert s_0) \, da_0:a_\infty P(s_0) ds_0 = \int_{s_0} V(s_0) P(s_0) ds_0\]<p>목적 함수는 Value function으로 표현이 가능하다.</p>\[V(s_0) = \int_{a_0:a_\infty} G_0 P(a_0:a_\infty \vert s_0) da_0:a_\infty\] \[= \int_{s_1:a_\infty} G_0 P(s_1: a_\infty \vert s_0, a_0) P(a_0 \vert s_0) d a_0 ds_1:a_\infty\] \[= \int_{a_0} Q(s_0, a_0) P(a_0 \vert s_0) da_0 = Q(s_0, a_{\theta,0})\]<p>V는 Q fuction으로 표현이 가능하다. 그리고 Action이 deterministic하기 때문에 \(\delta\) function이므로 적분이 사라지게 된다.</p>\[\int^\infty_\infty f(x) \delta(x-x_0) \,dx = f(x_0)\]<p>Q는 next Q로 표현이 가능하다.</p>\[Q(s_0, a_0) = \int_{s_1:a_\infty} (R_0 + \gamma G_1) P(s_1:a_\infty \vert s_0, a_0) \, ds_1:a_\infty\] \[= \int_{s_1:a_1} \int_{s_2:a_\infty} (R+\gamma G_1) P(s_1:a_\infty \vert s_0, a_0, s_1, a_1) P(s_1,a_1 \vert s_0, a_0) \, ds_1,a_1 \, ds_2:a_\infty\] \[=\int_{s_1,a_1} (R_0 + \gamma Q(s_1, a_1)) P(a_1 \vert s_1) P(s_1 \vert s_0, a_1) ds_1, a_1\] \[=R(s_0, a_{\theta,0}) + \int_{s_1} \gamma Q(s_1, a_{\theta,1}) P(s_1 \vert s_0, a_1) \, ds_1\]<p>우리는 목적함수를 최대화하기 위하며 미분을 한다.</p>\[\nabla_\theta J = \int_{s_0} \nabla_\theta V(s_0) P(s_0) \,ds_0 = \int_{s_0} \nabla_\theta Q(s_0, a_{\theta, 0} P(s_0) \,d s_0\]<p>\(\theta\)에 대해서 미분하기 위해 chain rule을 적용한다.</p><p>\(\nabla_\theta Q(s_0, a_{\theta,0}) = \nabla_{a_{\theta,0}} R(s_0, a_{\theta,0}) a_{\theta,0}' + \int \gamma Q(s_1, a_{\theta,1}) \nabla_{a_{\theta,0}} P(s_1 \vert s_0, a_{\theta,0}) a_{\theta,0}' \, ds_1\) \(+\int_{s_1} \gamma \nabla_\theta Q(s_1, a_{\theta,1}) P(s_1 \vert s_0, a_{\theta,0}) \, ds_1\)</p><p>\(= a_{\theta,0}' \nabla_{a_{\theta,0}} ( R_0 + \int_{s_1} \gamma Q(s_1, a_{\theta,1}) P(s_1 \vert s_0, a_{\theta,0}) \,ds_1)\) \(+ \int_{s_1} \gamma \nabla_\theta Q(s_1, a_{\theta,1}) P(s_1 \vert s_0, a_{\theta,0}) \, ds_1\) \(=a_{\theta,0}' \nabla_{a_{\theta,0}} Q(s_0, a_{\theta,0}) + \int_{s_1} \gamma \nabla_\theta Q(s_1, a_{\theta,1}) P(s_1 \vert s_0, a_{\theta,0}) \, ds_1\)</p><p>목적 함수를 미분하면 아래와 같다.</p><p>\(\nabla_\theta J = \int_{s_0} \nabla_\theta V(s_0) P(s_0) \,ds_0 = \int_{s_0} \nabla_\theta Q(s_0, a_{\theta, 0}) P(s_0) \,d s_0\) \(= \int_{s_0} a_{\theta,0}' \nabla_{a_{\theta,0}} Q(s_0,a_{\theta,0}) P(s_0) \, ds_0\) \(+ \int_{s_0} \int_{s_1} \gamma a_{\theta,0}' \nabla_{a_{\theta,1}}Q(s_1, a_{\theta,1})P(s_1 \vert s_0, a_{\theta,0})\,ds_1 P(s_0) \, ds_0\) \(+\int_{s_0} \int_{s_1} \int_{s_2} \gamma^2 a_{\theta,2} \nabla_{a_{\theta,2}}P(s_2 \vert s_1, a_{\theta,1}) \,ds_2 P(s_1) P(s_1 \vert s_0, a_{\theta,0}) \, ds_1 P(s_0) \, ds_0 + \cdots\)</p><p>\(=\underset{t=0}{\overset{\infty}{\sum}} \int_{s_0,s_1, \cdots s_t} \gamma^t a_{\theta,t}' \nabla_{a_{\theta,t}} Q(s_t, a_{\theta,t}) P(s_0) P(s_1 \vert s_0, a_{\theta,0}) P(s_2 \vert s_1, a_{\theta,1}) \cdots P(s_t \vert s_t, a_{\theta,t}) \, ds_0,s_1, \cdots s_t\) \(\approx \underset{t=0}{\overset{\infty}{\sum}} \int_{s_0,s_1, \cdots s_t} a_{\theta,t}' \nabla_{a_{\theta,t}} Q(s_t, a_{\theta,t}) P(s_0) P(s_1 \vert s_0, a_{\theta,0}) P(s_2 \vert s_1, a_{\theta,1}) \cdots P(s_t \vert s_t, a_{\theta,t}) \, ds_0,s_1, \cdots s_t\)</p><p>위 식이 update 식이다. DDPG는 sample을 구하기 위해 Q를 parameterize해서 사용한다. 위 식을 보면 policy가 존재하지 않고 transition만 존재하는 것을 확인할 수 있다. 그러므로 DDPG는 off-poliocy이다.</p><p>Actor를 update하기 위해서 Q를 사용하고 critic을 업데이트하기 위해서 TD error를 사용한다. 그리고 update 방식은 15 DQN과 같은 target network를 따로 두어 사용한다.</p><h1 id="additional">Additional</h1><p>이 논문에서는 서로 다른 scale의 feature를 위해서 batch normalization을 사용한다고 한다. 또한 network를 처음 initialize를 할 때 범위를 지정한 후 그 사이에서 normal distribution에서 sampling하여 사용한다고 한다.</p><h1 id="algorithm">Algorithm</h1><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/DDPG/algo.png" alt="algorithm" /> <em>Algorithm</em></p><p>DDPG는 exploration을 위해서 action에 Ornstein-Uhlenbeck process(OU)로 생성한 noise를 추가한다. OU noise는 평균으로 회귀하는 random process이라고 한다. 간단하게 normal distribution에 low variance로 설정한 noise를 사용해도 작은 task에 대해서는 괜찮은 것을 확인했다.</p><h1 id="reference">Reference</h1><ol><li><a href="https://arxiv.org/pdf/1509.02971.pdf">Lillicrap, Timothy P., et al. “Continuous control with deep reinforcement learning.” arXiv preprint arXiv:1509.02971 (2015).</a><li><a href="https://www.youtube.com/watch?v=cvctS4xWSaU&amp;list=PL_iJu012NOxehE8fdF9me4TLfbdv3ZW8g">혁펜하임 youtube</a></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/single-agent/'>Single-Agent</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/single-agent/" class="post-tag no-text-decoration" >Single-Agent</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning - JG_blog&url=https://leejungi.github.io/posts/DDPG/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning - JG_blog&u=https://leejungi.github.io/posts/DDPG/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning - JG_blog&url=https://leejungi.github.io/posts/DDPG/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DAGMM/">(Zong 2018 ICLR) Deep Autoencoding Gaussian Mixture Model For Unsupervised Anomaly Detection</a><li><a href="/posts/HIRO/">(Ofir 2018 Nips) Data-Efficient Hierarchical Reinforcement Learning</a><li><a href="/posts/FUN/">(Vezhnevets 2017 ICML) Feudal networks for hierarchical reinforcement learning</a><li><a href="/posts/ASAC/">(Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications</a><li><a href="/posts/SAC/">(Haarnoja 2018 ICML) Soft Actor-Critic; Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/single-agent/">Single-Agent</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/multi-agent/">Multi-Agent</a> <a class="post-tag" href="/tags/hierarchial-rl/">Hierarchial RL</a> <a class="post-tag" href="/tags/anomaly-detection/">Anomaly Detection</a> <a class="post-tag" href="/tags/distributed-rl/">Distributed RL</a> <a class="post-tag" href="/tags/memo/">memo</a> <a class="post-tag" href="/tags/meta-rl/">Meta-RL</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ASAC/"><div class="card-body"> <span class="timeago small" > Jun 27, 2021 <i class="unloaded">2021-06-27T01:50:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications</h3><div class="text-muted small"><p> 목차 Introduction Automating Entropy Adjustment for Maximum Entropy RL Algorithm Reference Introduction 이전 논문인 SAC에서 문제인 고정된 temperature의 문제를 해결하는 부분이 추가된 SAC base논문이다. optimal temperatur...</p></div></div></a></div><div class="card"> <a href="/posts/TD3/"><div class="card-body"> <span class="timeago small" > Jul 1, 2021 <i class="unloaded">2021-07-01T01:40:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Fujimoto 2018 ICML) Addressing Function Approximation Error in Actor-Critic Methods</h3><div class="text-muted small"><p> 목차 Introduction Clipped Double Q-Learning for Actor-Critic Target Networks and Delayed Policy Updates Target Policy Smoothing Regularization Algorithm Reference Introduction 이 논문은 D...</p></div></div></a></div><div class="card"> <a href="/posts/MDP/"><div class="card-body"> <span class="timeago small" > Mar 27, 2021 <i class="unloaded">2021-03-27T20:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>1. Markov Decision Process</h3><div class="text-muted small"><p> 목차 시작하기 전에 Markov Process Markov Decision Process Markov chain 용어 정리 Goal Reference 시작하기 전에 Reinforcement Learning(RL)을 공부하기 위해서 여러 책, 자료를 찾아봤고 이해하는 데 가장 큰 도움이 됬던 강의는 유투브 혁펜하임이라는 채...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/ACKTR/" class="btn btn-outline-primary" prompt="Older"><p>(Wu 2017 arxiv) Scalable trust-region method for deep reinforcementlearning using Kronecker-factored approximation</p></a> <a href="/posts/SAC/" class="btn btn-outline-primary" prompt="Newer"><p>(Haarnoja 2018 ICML) Soft Actor-Critic; Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">Jungi Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/single-agent/">Single Agent</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/multi-agent/">Multi Agent</a> <a class="post-tag" href="/tags/hierarchial-rl/">Hierarchial RL</a> <a class="post-tag" href="/tags/anomaly-detection/">Anomaly Detection</a> <a class="post-tag" href="/tags/distributed-rl/">Distributed RL</a> <a class="post-tag" href="/tags/memo/">memo</a> <a class="post-tag" href="/tags/meta-rl/">Meta RL</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://leejungi.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>

<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="(Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning" /><meta name="author" content="Jungi Lee" /><meta property="og:locale" content="en_US" /><meta name="description" content="목차" /><meta property="og:description" content="목차" /><link rel="canonical" href="https://leejungi.github.io/posts/MAAC/" /><meta property="og:url" content="https://leejungi.github.io/posts/MAAC/" /><meta property="og:site_name" content="JG_blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-04-22T01:30:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Jungi Lee" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Jungi Lee"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://leejungi.github.io/posts/MAAC/"},"description":"목차","url":"https://leejungi.github.io/posts/MAAC/","@type":"BlogPosting","headline":"(Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning","dateModified":"2021-04-23T01:33:39+09:00","datePublished":"2021-04-22T01:30:00+09:00","@context":"https://schema.org"}</script><title>5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning | JG_blog</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/carrot.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JG_blog</a></div><div class="site-subtitle font-italic">RL researcher and developer</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/leejungi" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ganbbang12','naver.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Apr 22, 2021, 1:30 AM +0900" > Apr 22, 2021 <i class="unloaded">2021-04-22T01:30:00+09:00</i> </span> by <span class="author"> Jungi Lee </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Fri, Apr 23, 2021, 1:33 AM +0900" > Apr 23, 2021 <i class="unloaded">2021-04-23T01:33:39+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2104 words">11 min</span></div></div><div class="post-content"><h1 id="목차">목차</h1><ol><li><a href="#problem-definition">Problem definition</a><li><a href="#background">Background</a><li><a href="#multi-actor-attention-criticmaac">Multi-Actor-Attention-Critic(MAAC)</a><li><a href="#algorithm">Algorithm</a><li><a href="#experiment">Experiment</a><li><a href="#review-comment">Review comment</a><li><a href="#reference">Reference</a></ol><h1 id="problem-definition">Problem definition</h1><p>이전 Multi-agent Reinforcement learning을 위해서 하는 가장 간단한 방법은 독립적인 agent를 두고 학습하는 방법이지만 이 방법은 환경이 stationary하고 markovian이여야하는 가정을 위반한다. 그래서 다른 방법으로 모든 agent로부터 정보를 얻는 centrally critic을 두는 방법이다. 하지만 이 알고리즘은 여러 agent를 학습하고 다루기에 어렵다는 문제가 있다. 그래서 이 논문은 attention mechanism을 이용하여 centralized critic을 학습하는 방법을 제안한다. 이 방법으로 인하여 각 agent는 특정 다른 agent의 정보에 집중하여 정보를 얻어 학습할 수 있게 된다.</p><h1 id="background">Background</h1><h2 id="attention-models">Attention models</h2><p>이 논문에서는 <a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention models</a>을 이용하여 critic을 만드는 방법을 제안한다. 이전에 Seq2Seq 구조에서 많이 사용하던 방법이다. 논문에 부족하던 background를 간단히 설명하기 위해서 Attention model의 기초가 되는 방법부터 설명을 하겠다.</p><p>간단한 예제로 기초 방법을 설명하겠다. 해당 내용은 <a href="https://www.youtube.com/watch?v=NSjpECvEf0Y&amp;t=1957s">video</a>를 참고하여 만들었다.</p><p>Key, query, value 세 가지 파라미터가 있다. Query의 input을 받았을 때 key값과 유사도를 계산한다. 이후 유사도와 value 값을 곱하여 output을 만든다. 즉 유사도를 이용하여 해당 value의 어떤 값에 집중(attention)을 해야하는 지 계산한다.</p><p>example)</p><p>Query: ‘유재석’<br /> dictionary(key:value): {‘유재석’: ‘지미유’, ‘이효리’: ‘천옥’, ‘엄정화’: ‘만옥’}</p><p>Query로 ‘유재석’이 들어 왔을 때 dictionary를 이용하여 유사도를 판단한다. 유사도는 {1,0,0}이 나올 것이고 이 유사도를 dictionary의 value와 곱하면 output으로 지미유가 나온다. 아래는 해당 과정을 코드로 나타낸 것이다.</p><ol><li>Similarity(query, key): key와 query 유사도(sim) 계산<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">Similarity</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
 <span class="k">if</span> <span class="n">query</span> <span class="o">==</span> <span class="n">key</span><span class="p">:</span>
     <span class="k">return</span> <span class="mi">1</span>
 <span class="k">else</span><span class="p">:</span>
     <span class="k">return</span> <span class="mi">0</span>
</pre></table></code></div></div><li>SimXValue(sim, value): 유사도와 value를 곱한다.<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">SimXValue</span><span class="p">(</span><span class="n">sim</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
 <span class="n">output</span> <span class="o">=</span> <span class="n">sim</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
 <span class="k">return</span> <span class="n">output</span>
</pre></table></code></div></div><li>Result: 유사도와 value를 곱한 값의 합을 return<br /> \(\text{result} = \underset{i}{\sum} \text{similarity(key,query)} * \text{value}\)<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">Result</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
 <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</pre></table></code></div></div></ol><p>위 방법은 dictionary 형태로 attention model은 위와 비슷한 방법이다. 우선 query와 key의 유사도를 계산한 후 value의 가중합을 계산한다. 아래 식은 attention model의 수식을 나타낸다.</p>\[A(q,k,v) = \underset{i}{\sum} \text{softmax}(f(k,q))v\]<p>위에서 f는 similarity function으로 dot product나 scaled dot product 등을 이용한다.</p><p>Dot product<br /> Attention score(similarity) = \(\sum Q_i * K_i\)</p><p>Scaled dot product <br /> Attention score(similarity) = \(\frac{1}{\sqrt{n}}\sum Q_i * K_i\)</p><p>더 자세한 설명은 이 <a href="https://www.youtube.com/watch?v=NSjpECvEf0Y&amp;t=1957s">video</a>를 참고하여 이해하면 될 것같다. 이전에도 attention model을 이용한 Multi-agent 논문이 있었지만 이 논문은 policy사이의 공유 정보에 초점을 두었기 때문에 더 좋은 성능 보여준다고 말한다.</p><h2 id="soft-actor-critic">Soft Actor-Critic</h2><p><a href="https://arxiv.org/pdf/1801.01290.pdf">Soft Actor-Critic(SAC)</a>는 Policy gradient 방법의 대표적으로 좋은 성능을 보여주는 논문으로 policy graident에 entropy 개념을 도입하여 방법이다. Entropy를 이용하여 exploration을 더 잘할 수 있도록 하였다. 이 논문의 key idea와 구조만을 본다고 한다면 안 읽어도 될 것 같다.</p><p>SAC의 policy gradient 공식은 아래와 같다.</p><p>\(L_Q(\psi) = E_{(s,a,r,s') \sim D} [(Q_{\psi}(s,a) - y)^2]\)<br /> where, \(y = r(s,a) + \gamma E_{a' \sim \pi(s')}[Q_{\bar{\psi}}(s',a')]\)</p><p>\(\nabla_{\theta} J(\pi_{\theta}) = E_{s \sim D, a \sim \pi}[\nabla_{\theta} log(\pi_{\theta}(a \vert s)) (-\alpha log(\pi_{\theta}(a \vert s)) + Q_{\psi}(s,a) - b(s)]\)<br /> where b(s) is a state-depedent baseline(V)</p><h1 id="multi-actor-attention-criticmaac">Multi-Actor-Attention-Critic(MAAC)</h1><h2 id="notion">Notion</h2><ul><li>j: the set of all agents except i as \i<li>\(Q_i^{\psi}(o,a)\): agent i의 observation과 action와 다른 agent의 정보를 이용하며 만든 함수</ul>\[Q_i^{\psi}(o,a) = f_i(g_i(o_i,a_i), x_i)\]<p>where \(f_i\) is a 2-layer multi-layer perceptron(MLP), \(g_i\) 1-layer MLP</p><ul><li>\(x_i\): contribution from toher agents</ul>\[x_i = \underset{j \neq i}{\sum} \alpha_jv_j = \underset{j \neq i}{\sum} \alpha_j h(Vg_j(o_j,aj))\]<p>where \(v_j\) is a function of agent j’s embedding, encoded with an embedding function and then linearly transformed by a shared matrix V, h is leaky ReLU.</p><ul><li>Attention score(\(\alpha_j\))</ul>\[\alpha_j \propto \text{exp} (e_j^T W_k^T W_q e_i)\]<p>MAAC의 critic 구조는 아래 그림과 같다. observation과 action을 이용하여 e를 만든다.(특징 추출) \(e_i\)와 \(e_j\)를 query와 key로 유사도를 측정을 하고 value와 곱하여 contribution을 계산한다. 이를 이용하여 agent i가 어떤 agent의 정보를 얼마나 이용해야하는 지 계산하게 된다. 이 값들은 학습을 통하여 설정된다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MARL/MAAC/architecture.png" alt="architecture" /> <em>MAAC Architecture</em></p><p>Actor와 Critic의 update는 위의 SAC 공식을 이용하여 update하게 된다.</p><p>\(L_Q(\psi) = E_{(s,a,r,s') \sim D} [(Q_{\psi}(s,a) - y)^2]\)<br /> where, \(y = r(s,a) + \gamma E_{a' \sim \pi(s')}[Q_{\bar{\psi}}(s',a')]\)</p><p>\(\nabla_{\theta} J(\pi_{\theta}) = E_{s \sim D, a \sim \pi}[\nabla_{\theta} log(\pi_{\theta}(a \vert s)) (-\alpha log(\pi_{\theta}(a \vert s)) + Q_{\psi}(s,a) - b(s)]\)<br /> where b(s) is a state-depedent baseline(V)</p><p>b(s)는 아래와 같은 공식으로 표현된다.</p>\[b(o,a_{\backslash i}) = E_{a_i \sim \pi_i(o_i)}[Q_i^\psi(o,(a_i,a_{\backslash i}))] = \underset{a'_i \in A_i}{\sum} \pi(a'_i \vert o_i) Q_i(o, (a'_i, a_{\backslash i}))\]<p>\(Q_i\)에서 agent i의 action을 제거하고 모든 action의 value를 사용하였다. 그리고 위 공식을 사용하기 위해 \(e_i=g_i(o_i,a_i)\) 대신 \(e_i=g^o_i(o_i)\)를 사용하였다.</p><h1 id="algorithm">Algorithm</h1><p>MAAC의 알고리즘은 아래와 같다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MARL/MAAC/algo1.png" alt="algo1" /> <em>MAAC Algorithm</em></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MARL/MAAC/algo2.png" alt="algo2" /> <em>Update Actor Critic</em></p><h1 id="experiment">Experiment</h1><p>이 논문에서는 실험 결과를 입증하기 위해 이전과 같은 starcraft2 환경이 아닌 다른 두 환경을 이용하였다. 첫 번째는 Cooperative Treausre Collection(CTC)이라는 환경으로 6 개의 treasure hunter와 2 개의 treasure bank(총 8개의 agent)로 이루어져 잇으며 treasure hunter는 treasure를 모으고 treasure bank에 treasure를 내보낸다. 두 번째는 Rover-Tower로 한 pair로 이루어진 rover와 tower가 있고 rover는 tower의 정보만을 이용하여 goal을 목표로 이동한다. tower는 탐사 로봇을 포함하여 목적지를 찾을 수 있고 paired 된 탐사 로봇에게 5개의 메시지(상,하,좌,우,정지?) 중 하나를 보낼 수 있다.</p><p>아래는 다른 multi-agent 방법과 비교한 결과이다. MAAC(uniform)은 attention score를 동일한 비율(uniform, 1/N)으로 한 결과를 의미한다. MAAC는 두 환경 모두 좋은 결과를 보여주고 Rover-tower의 경우 MAAC(uniform)이 성능이 급격히 하락하는 것을 볼 수 있다. 이는 Rover-tower 실험이 attention model가 어떤 효과를 보여주는 지 입증하는 실험이라는 것을 알 수 있게 해준다. COMA 논문은 비슷한 action space를 가지는 경우에는 좋은 결과를 보여주는 것을 확인할 수 있었고 MADDPG의 경우 agent에 따라 큰 observation space를 가져야하기 때문에 CTC 환경에서 좋지 않은 결과를 보여주는 것을 확인할 수 있었다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MARL/MAAC/result1.png" alt="result1" /> <em>Performance comparing with others</em></p><p>MADDPG와 agent가 증가함에 따른 실험 결과를 비교하면 MAAC는 비슷한 성능을 보여주지만 MADDPG는 증가함에 따라 성능이 낮아지는 것을 아래 그림에서 확인할 수 있다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MARL/MAAC/result2.png" alt="result2" /> <em>Scalability in the Rover-Tower task</em></p><p>Rover Tower의 경우 tower의 어떤 정보에 attention 해야하는 지가 중요하게 작용하는 부분이 있다. 아래 그림은 tower 3와 pairing된 rover 1이 어느 tower에 attention을 하고 있는 지 attention score를 보여주는 그림이고 pairing된 tower3에 가장 높은 attetnion score를 가지는 것을 확인할 수 있다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/MARL/MAAC/result3.png" alt="result3" /> <em>Attention weights over all Towers</em></p><h1 id="review-comment">Review comment</h1><p>이전 posting의 논문들인 LIIR이나 QMIX 논문과 비교 내용이 없다는 것이 아쉽다. Attention 알고리즘이 가지는 의미와 효과를 실험적으로 잘 보여준 것같고 범용적인 multi-agent 환경에서 좋게 쓰일 수 있는 지에대해서는 추가적인 실험과 LIIR, QMIX 논문과 비교를 하는 것이 좋을 것같다. Attention 알고리즘을 적용하였다는 새로운 아이디어를 적용했다는 것에 큰 의미를 가지는 것같다. 이전 논문과 다르게 다른 agent의 정보를 이용하기 때문에 더 좋은 결과를 보여주는 것 같다.</p><h1 id="reference">Reference</h1><ol><li><a href="https://arxiv.org/pdf/1810.02912.pdf">Iqbal, Shariq, and Fei Sha. “Actor-attention-critic for multi-agent reinforcement learning.” International Conference on Machine Learning. PMLR, 2019.</a><li><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Vaswani, Ashish, et al. “Attention is all you need.” arXiv preprint arXiv:1706.03762 (2017).</a><li><a href="https://www.youtube.com/watch?v=NSjpECvEf0Y&amp;t=1957s">DMQA Open Seminar; Graph Attention Networks</a><li><a href="https://arxiv.org/pdf/1801.01290.pdf">Haarnoja, Tuomas, et al. “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.” International Conference on Machine Learning. PMLR, 2018.</a></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/multi-agent/'>Multi-Agent</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/multi-agent/" class="post-tag no-text-decoration" >Multi-Agent</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning - JG_blog&url=https://leejungi.github.io/posts/MAAC/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning - JG_blog&u=https://leejungi.github.io/posts/MAAC/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning - JG_blog&url=https://leejungi.github.io/posts/MAAC/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DAGMM/">(Zong 2018 ICLR) Deep Autoencoding Gaussian Mixture Model For Unsupervised Anomaly Detection</a><li><a href="/posts/HIRO/">(Ofir 2018 Nips) Data-Efficient Hierarchical Reinforcement Learning</a><li><a href="/posts/FUN/">(Vezhnevets 2017 ICML) Feudal networks for hierarchical reinforcement learning</a><li><a href="/posts/ASAC/">(Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications</a><li><a href="/posts/SAC/">(Haarnoja 2018 ICML) Soft Actor-Critic; Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/single-agent/">Single-Agent</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/multi-agent/">Multi-Agent</a> <a class="post-tag" href="/tags/hierarchial-rl/">Hierarchial RL</a> <a class="post-tag" href="/tags/anomaly-detection/">Anomaly Detection</a> <a class="post-tag" href="/tags/distributed-rl/">Distributed RL</a> <a class="post-tag" href="/tags/memo/">memo</a> <a class="post-tag" href="/tags/meta-rl/">Meta-RL</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/MARL-1993/"><div class="card-body"> <span class="timeago small" > Mar 28, 2021 <i class="unloaded">2021-03-28T01:27:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>1. (Tan 1993 ICML) Multi-Agent Reinforcement Learning; Independent vs Cooperative Agents</h3><div class="text-muted small"><p> 목차 1. Introduction 2. 환경 3. Case1: sharing sensation 4. Case2: sharing policies or episodes 5. Case3: on joint tasks 6. Concolusion 7. Reference Introduction 사람들이 일을 하기 위해서는 분업을 하고 있고 곤충들 또한 그렇다. ...</p></div></div></a></div><div class="card"> <a href="/posts/COMA/"><div class="card-body"> <span class="timeago small" > Apr 4, 2021 <i class="unloaded">2021-04-04T04:30:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>2. (Foerster 2017 AAAI) Counterfactual Multi-Agent Policy Gradients</h3><div class="text-muted small"><p> 목차 1. Main ideas 2. Notation 3. Indepedent Actor-Critic 4. Counterfactual Multi-Agent Policy Gradients 5. COMA Lemma 1 6. Algorithm 7. Reference Main ideas Centralised critic 사용 Counterfactua...</p></div></div></a></div><div class="card"> <a href="/posts/QMIX/"><div class="card-body"> <span class="timeago small" > Apr 11, 2021 <i class="unloaded">2021-04-11T02:10:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>3. (Rashid 2018 ICML) Qmix; Monotonic value function factorisation for deep multi-agent reinforcement learning</h3><div class="text-muted small"><p> 목차 Problem definition Related work QMIX Representational Complexity Experiment Review comment Reference Problem definition Agent의 적절한 action을 위해서 centralised action-value functi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Dueling-DQN/" class="btn btn-outline-primary" prompt="Older"><p>8. Dueling DQN</p></a> <a href="/posts/PER/" class="btn btn-outline-primary" prompt="Newer"><p>9. Prioritized Experience Replay</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">Jungi Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/single-agent/">Single Agent</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/multi-agent/">Multi Agent</a> <a class="post-tag" href="/tags/hierarchial-rl/">Hierarchial RL</a> <a class="post-tag" href="/tags/anomaly-detection/">Anomaly Detection</a> <a class="post-tag" href="/tags/distributed-rl/">Distributed RL</a> <a class="post-tag" href="/tags/memo/">memo</a> <a class="post-tag" href="/tags/meta-rl/">Meta RL</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://leejungi.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>

<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning" /><meta name="author" content="Jungi Lee" /><meta property="og:locale" content="en_US" /><meta name="description" content="목차 Problem definition Background Shannon-Gibbs entropy q-Expoential,q-Logarithm and Tsallis Entropy Proposition 1 Proposition 2 Bandit with Maximum Tsallis Entropy Proposition 3 Proposition 4 q-Maximum Theorem 1 Maximum Tsallis Entropy in MDPS Proposition 5,6 and lemma 9.1 Lemma 9.2 Tsallis Bellman Optimality Equation Theorem 9 Dynamic Programming for Tsallis MDPs Tsallis Policy Iteration Theorem 10 Theorem 11, 12 Tsallis Value Iteration Performance Error Bounds Theorem 14 Tsallis Actor Critic for Model-Free RL Experiment Reference" /><meta property="og:description" content="목차 Problem definition Background Shannon-Gibbs entropy q-Expoential,q-Logarithm and Tsallis Entropy Proposition 1 Proposition 2 Bandit with Maximum Tsallis Entropy Proposition 3 Proposition 4 q-Maximum Theorem 1 Maximum Tsallis Entropy in MDPS Proposition 5,6 and lemma 9.1 Lemma 9.2 Tsallis Bellman Optimality Equation Theorem 9 Dynamic Programming for Tsallis MDPs Tsallis Policy Iteration Theorem 10 Theorem 11, 12 Tsallis Value Iteration Performance Error Bounds Theorem 14 Tsallis Actor Critic for Model-Free RL Experiment Reference" /><link rel="canonical" href="https://leejungi.github.io/posts/tac/" /><meta property="og:url" content="https://leejungi.github.io/posts/tac/" /><meta property="og:site_name" content="JG_blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-05-02T22:10:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Jungi Lee" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Jungi Lee"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://leejungi.github.io/posts/tac/"},"description":"목차 Problem definition Background Shannon-Gibbs entropy q-Expoential,q-Logarithm and Tsallis Entropy Proposition 1 Proposition 2 Bandit with Maximum Tsallis Entropy Proposition 3 Proposition 4 q-Maximum Theorem 1 Maximum Tsallis Entropy in MDPS Proposition 5,6 and lemma 9.1 Lemma 9.2 Tsallis Bellman Optimality Equation Theorem 9 Dynamic Programming for Tsallis MDPs Tsallis Policy Iteration Theorem 10 Theorem 11, 12 Tsallis Value Iteration Performance Error Bounds Theorem 14 Tsallis Actor Critic for Model-Free RL Experiment Reference","url":"https://leejungi.github.io/posts/tac/","@type":"BlogPosting","headline":"(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning","dateModified":"2021-05-08T22:52:26+09:00","datePublished":"2021-05-02T22:10:00+09:00","@context":"https://schema.org"}</script><title>(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning | JG_blog</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/carrot.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">JG_blog</a></div><div class="site-subtitle font-italic">RL researcher and developer</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/leejungi" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ganbbang12','naver.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, May 2, 2021, 10:10 PM +0900" > May 2, 2021 <i class="unloaded">2021-05-02T22:10:00+09:00</i> </span> by <span class="author"> Jungi Lee </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sat, May 8, 2021, 10:52 PM +0900" > May 8, 2021 <i class="unloaded">2021-05-08T22:52:26+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5889 words">32 min</span></div></div><div class="post-content"><h1 id="목차">목차</h1><ol><li><a href="#1-problem-definition">Problem definition</a><li><a href="#2-background">Background</a><ul><li><a href="#shannon-gibbs-entropy">Shannon-Gibbs entropy</a><li><a href="#q-exponential-q-logarithm-and-tsallis-entropy">q-Expoential,q-Logarithm and Tsallis Entropy</a><ul><li><a href="#proposition-1">Proposition 1</a><li><a href="#proposition-2">Proposition 2</a></ul></ul><li><a href="#3-bandit-with-maximum-tsallis-entropy">Bandit with Maximum Tsallis Entropy</a><ul><li><a href="#proposition-3">Proposition 3</a><li><a href="#proposition-4">Proposition 4</a><li><a href="#q-maximum">q-Maximum</a><ul><li><a href="#theorem-1">Theorem 1</a></ul></ul><li><a href="#4-maximum-tsallis-entropy-in-mdps">Maximum Tsallis Entropy in MDPS</a><ul><li><a href="#proposition-56-and-lemma-91">Proposition 5,6 and lemma 9.1</a><li><a href="#lemma-92">Lemma 9.2</a><li><a href="#tsallis-bellman-optimality-equation">Tsallis Bellman Optimality Equation</a><ul><li><a href="#theorem-9">Theorem 9</a></ul></ul><li><a href="#5-dynamic-programming-for-tsallis-mdps">Dynamic Programming for Tsallis MDPs</a><ul><li><a href="#tsallis-policy-iteration">Tsallis Policy Iteration</a><ul><li><a href="#theorem-10">Theorem 10</a><li><a href="#theorem-11-12">Theorem 11, 12</a></ul><li><a href="#tsallis-value-iteration">Tsallis Value Iteration</a><li><a href="#performance-error-bounds">Performance Error Bounds</a><ul><li><a href="#theorem-14">Theorem 14</a></ul></ul><li><a href="#tsallis-actor-critic-for-model-free-rl">Tsallis Actor Critic for Model-Free RL</a><li><a href="#experiment">Experiment</a><li><a href="#reference">Reference</a></ol><h1 id="1-problem-definition">1. Problem definition</h1><p>이전에 RL의 exploration을 더 잘하기 위해서 entropy 개념을 도입하여 최적화하였다. 그 대표적인 entropy가 Shannon-Gibbs(SG) entropy이다. 역설적으로 exploration을 더 잘하게 만들어서 exploit이 감소하여 performance가 감소하는 문제도 있다. 이를 해결하기 위해 entropy decaying과 같은 방법을 통해 해결하였다. 다른 방법으로는 sparse Tsallis(ST) entropy를 사용하는 방법이있다. ST entropy는 exploration을 더 적게하여 suboptimal policy를 찾을 수 있다는 문제가 있다. 그래서 해당 논문에서는 Tsallis MDP문제로 정의하여 다양한 entropy를 cover할 수 있도록 제안한다.</p><h1 id="2-background">2. Background</h1><h2 id="shannon-gibbs-entropy">Shannon-Gibbs entropy</h2>\[S = - \overset{n}{\underset{i=1}{\sum}} p_i \operatorname{log}p_i\]<p>이 entropy를 maximize하는 의미는 \(p_i\)를 모든 state에 대해서 같은 확률로 만든다는 의미이다.</p>\[- \overset{n}{\underset{i=1}{\sum}} p_i \operatorname{log}p_i \leq - \overset{n}{\underset{i=1}{\sum}} \frac{1}{n} \operatorname{log}{\frac{1}{n}}\]<h2 id="q-exponential-q-logarithm-and-tsallis-entropy">q-Exponential, q-Logarithm and Tsallis Entropy</h2>\[\operatorname{exp_q}(x) \triangleq \begin{cases} \operatorname{exp}(x) &amp;\text{if q = 1}\\ [1+(q-1)x]^{\frac{1}{q-1}}_+ &amp;\text{if q} \neq \text{1}\end{cases}\] \[\operatorname{ln}_q(x) \triangleq \begin{cases} \operatorname{log}(x) &amp;\text{if q = 1 and x &gt; 0}\\ \frac{x^{q-1}-1}{q-1} &amp;\text{if q} \neq \text{1 and x &gt; 0}\end{cases}\]<p>where \([x]_+ = \operatorname{max}(x,0)\) and q is a real number</p><p>위 공식은 q-exponential, q-logarithm 공식이다. \(\operatorname{ln}_q(x)\)는 x에대해 단조 증가함수이다. q가 1일 경우 log(x)가 되며 q가 2일 경우 선형 함수가 된다. 하지만 q&gt;2일 경우 \(d\operatorname{ln}_q (x)/dx\)는 증가, q&lt;2일 경우 \(d\operatorname{ln}_q (x)/dx\)는 감소 함수가 된다.</p><p>Tsallis entorpy는 \(\operatorname{ln}_q(x)\)로 표현할 수 있다.</p>\[S_q(P) \triangleq \underset{X \sim P}{E} [-\operatorname{ln}_q(P(X))]\]<p>q가 1일 경우 SG entropy가 되고 q가 2일 경우 ST entropy가 된다. q가 무한대일 경우 entropy는 0이 된다. q&gt;0일 경우 Tsallis entropy는 concave function이 된다.</p><h2 id="proposition-1">Proposition 1</h2><details> <summary> Concave 증명</summary><div><p>Assume that X is a finite space. Let P is a probability distribution over X. if q&gt;0, then, \(S_q(P)\) is concave with respect to P.</p><p>proof. \(f(x) = -x \operatorname{ln}_q(x)\)이라고 정의(x&gt;0). 이를 두번 미분.</p><p>1번 미분<br /> \(\frac{df(x)}{dx} = -\operatorname{ln}_q(x) -x\operatorname{ln}_q^{'}(x)\)</p><p>2번 미분<br /> \(\frac{d^2f(x)}{dx^2} = -\operatorname{ln}_q^{'}(x) -\operatorname{ln}_q^{'}(x) -\operatorname{ln}_q^{''}(x) = -2 \frac{(q-1)x^{q-2}}{q-1} - x\frac{(q-1)(q-2)x^{q-3}}{q-1}\) \(= -2x^{q-2}-x(q-2)x^{q-3} = -qx^{q-2}\)</p>\[\frac{d^2f(x)}{dx^2} = -qx^{q-2} &lt;0(x&gt;0, q&gt;0)\]<p>-q는 항상 음수이고 x의 지수함수는 항상 양수이기 때문에 2번 미분한 값은 항상 음수로 나온다. 이 결과를 이용하여 다음과 같은 특성이 만족하는 것을 보일 수 있다.</p><p>\(\lambda_1, \lambda_2 \geq 0\), \(\lambda_1+\lambda_2 =1\)일 경우 probability \(P_1, P_2\)에 대해서 아래의 식을 만족한다.(중간값 정리)</p><p>\(S_q(\lambda_1P_1 + \lambda_2P_2) = \underset{x}{\sum}-(\lambda_1P_1(x) + \lambda_2P_2(x))\operatorname{ln}_q(\lambda_1P_1(x)+\lambda_2P_2(x))\) \(\ge \underset{x}{\sum} -\lambda_1P_1(x)\operatorname{ln}_q(P_1(x)) - \lambda_2P_2(x) \operatorname{ln}_q(P_2(x))\) \(=\lambda_1S_q(P_1) + \lambda_2S_q(P_2)\)</p><p>*위 식의 부등식(&gt;)은 논문 부등식(&lt;) 반대로 되어있다. 내가 생각하였을 때 중간값 정리를 적용하면 논문과 다르게 부등식이 반대로 되어야한다고 생각된다. 또한 뒤쪽에서 위 식을 사용하는 부분에서는 부등식이 반대로 되어 있기 때문에 &gt; 부등식을 사용하는 것이 맞다고 생각한다.</p><p>그러므로 \(S_q(P)\)는 P에 대해서 concave하다.</p></div></details><h3 id="proposition-2">Proposition 2</h3><details> <summary> Entropy가 최대가되는 P 값 증명</summary><div><p>Assume that X is a finite space. Then, S_q(P) is maximized when P is a uniform distribution. i.e., P= 1|X| where |x| is the number of elements in X.</p><p>proof. KKT condition을 이용하여 optimization problem 해결</p><p>기본적으로 discrete uniform distribution일 때 entropy가 최대값을 가진다는 것을 알고 있다. 이 논문에서는 다시 한번 증명을 하고 있고 이 내용을 후반 부 증명에서 사용했다.</p>\[\underset{P \in \triangle}{\operatorname{max}} S_q(P)\]<p>where, \(\triangle = \{P \mid P(x) \geq 0, \underset{x}{\sum} P(x) =1\}\) is a probability simplex.</p><p>KKT condition:</p><p>\(\forall x \in X, \frac{\partial(S_q(\pi)-\underset{x}{\sum}\lambda^*(x)P(x) - \mu^*(1-\underset{x}{\sum}P(X)))}{\partial P(x)} \bracevert_{P(x)=P^*(x)}\) \(= -\operatorname{ln}_q(P^*(x))-(P^*(x))^{q-1}-\lambda^*(x) + \mu^*\) \(=-q \operatorname{ln}_q(P^*(x)) -1 -\lambda^*(x) + \mu^* =0\)<br /> \(\forall x \in X, 0 = 1 -\underset{x}{\sum}P^*(x), P^*(x) \geq 0\)<br /> \(\forall x \in X, \lambda^*(x) \leq 0\)<br /> \(\forall x \in X, \lambda^*(x)P^*(x) =0\)</p><p>where \(\lambda^*\) and \(\mu^*\) are the Lagrangian multipliers for constraints in \(\triangle\)</p><p>세부 증명 1.<br /> \(\frac {\partial S_q(\pi)} {\partial P(x)} = \frac{P(x)^{q-1}-1}{q-1} + P(x) \frac{(q-1)P(x)^{q-2}}{q-1}\)<br /> \(=\frac{P(x)^{q-1}-1}{q-1} + \frac{(q-1)P(x)^{q-1}}{q-1}\) \(=\frac{qP(x)^{q-1}-1}{q-1}\) \(=\frac{qP(x)^{q-1} -q -1 +q}{q-1}\) \(=q\operatorname{ln}_qP(x)-1\)</p><p>세부 증명 2. 첫번째 KKT condition을 이용하여 \(P^*(x)\)에 대해서 정리<br /> \(P^*(x) \ge 0\)이므로 \(\lambda^*(x) =0\)이 된다.</p><p>\(-q\operatorname{ln}_q(P^*(x)) -1 + \mu^*=0\)<br /> \(\operatorname{ln}_q(P^*(x)) = \frac{-1+\mu^*}{q}\)<br /> \(\frac{P^*(x)^{q-1}-1}{q-1} = \frac{-1 + \mu^*}{q}\)<br /> \(P^*(x)^{q-1}-1 = (q-1) \frac{-1 + \mu^*}{q}\)<br /> \(P^*(x) = (1+(q-1)\frac{-1 + \mu^*}{q})^{\frac{1}{q-1}}=\operatorname{exp}_q(\frac{\mu^* -1}{q})\)</p><p>P는 \(\mu\)와 q에 의해서만 결정된다.(constant probability mass) x에 대한 함수가 아니므로 \(P^*(x)=1/\vert S \vert\) where \(S=\{x \mid P^*(x) \ge0\}\)이다.(P가 모든 확률에 대한 uniform이 아닌 일부 확률에 대해서만 동일한 확률을 가질 수 있다. 물론 \(P^*(x) \ge 0\)이라고 condition이 있으므로 이미 uniform이라고 해도 될 것같다.)</p><p>Optimal value는 \(S_q(P^*)=-\operatorname{ln}_q(1/ \vert S \vert)\)이다. \(-\operatorname{ln}_q(x)\)는 단조 증가함수이므로 \(\mid S \mid\)는 최대가 되어야한다. S=X일 때 S가 최대가 되므로 \(P^*(x) = 1/\vert X \vert\)이다.</p></div></details><h1 id="3-bandit-with-maximum-tsallis-entropy">3. Bandit with Maximum Tsallis Entropy</h1><p>MDP에 적용하기 전에 간단한 예제인 multi-armed bandit(MAB) 문제에 적용을 한다. MAB 문제에서 정의한 특성들이 MDP 문제에도 적용을 할 수 있다.</p>\[\underset{\pi \in \triangle}{\operatorname {max}} [\underset{a \sim \pi}{E}[R] + \alpha S_q(\pi)]\]<p>MAB의 Tsallis entropy maximization은 위 식으로 정의된다.</p>\[\pi_q^*(a) = \underset{a}{\sum} \operatorname{exp}_q (r(a)/q- \psi(r/q))\]<p>Tsallis entroy의 stochastic optimal policy는 위 식으로 정의된다. \(\psi\)는 q-potential function으로 q-expoential, q-logarithm과 같은 q가 변수로 사용되는 function이다.</p>\[\underset{a \sim \pi^*}{E}[R] + \alpha S^*_q(\pi)=(q-1) \underset{a}{\sum}\frac{r(a)}{q} \operatorname{exp}_q (\frac{r(a)}{q} - \psi_q(\frac{r}{q})) + \psi_q (\frac{r}{q})\]<p>\(\pi^*_q\)를 이용한 optimal value는 위 식으로 정의된다. q가 1일 경우 optimal policy는 softmax distribution이 된다. q가 2일 경우 ST entropy를 이용한 optimal policy가 된다. \(\pi^*_1\)과 다르게 \(\pi^*_2\)는 0 probability를 만들 수 있다.(\(r(a) &lt; 2\psi_2(r/2) -2\)일 경우) q가 무한대일 경우 기본적인 MAB problem이 된다. q가 1,2, 무한대 일경우 \(\psi_q\)구하기 쉬웠지만 이외의 경우는 어렵다. 다른 논문에서 이를 first order tayler expansion으로 구하는 방법을 소개했지만 이 논문에서는 다른 방법을 사용한다.(section 6에서 다뤄진다. network를 이용하여 update하는 방법을 사용)</p><h2 id="proposition-3">Proposition 3</h2><details> <summary> Optimal solution of MAB with maximum Tsallis entropy</summary><div><p>The optimal solution of</p>\[\underset{\pi \in \triangle}{\operatorname{max}} [ \underset{a \sim \pi}{E} [R] + S_q(\pi)]\]<p>is</p>\[\pi^*_q(a) = \operatorname{exp}_q(\frac{r(a)}{q} - \psi_q(\frac{r}{q}))\]<p>where the q-potential function \(\psi_q\) is a functional define on {A,r}</p><p>\(\psi\)는 아래의 공식의 condition에 의해서 결정된다.</p>\[\underset{a}{\sum}\pi^*_q(a) = \underset{a}{\sum} \operatorname{exp}_q (\frac{r(a)}{q} - \psi (\frac{r}{q})) =1\]<p>\(\psi^*_q\)를 이용해서 optimal value는 아래와 같이 정의할 수 있다.</p>\[\underset{\pi \in \triangle}{\operatorname{max}} [ \underset{a \sim \pi}{E} [R] + S_q(\pi)] = (q-1) \underset{a}{\sum} \frac{r(a)}{q} \operatorname{exp}_q(\frac{r(a)}{q} - \psi_q \frac{r}{q}) + \psi_q(\frac{r}{q})\]<p>proof. \(\operatorname{exp}_q \in [0, \infty)\)는 continous 단조 함수 이기 때문에 \(\underset{a}{\sum} \operatorname{exp}_q(\frac{r}{q} - \xi)\)는 0으로 수렴하고 \(\xi\)가 +,- 무한대일 경우 무한대로 수렴한다. 그러므로 \(\underset{a}{\sum} \operatorname{exp}_q(\frac{r}{q} - \xi^*)=1\)을 만족하는 \(\xi^*\)가 존재한다. 그러므로 \(\psi_q(r/q) = \xi^*\)라고 할 수 있다.</p><p>나머지는 KKT condition을 고려하여 구한다. KKT conditon:</p><p>\(\forall i, 1-\underset{a}{\sum} \pi^*_q(a) = 0, \pi^*_q(a) \geq 0\)<br /> \(\forall i, \lambda^*(a) \leq 0\)<br /> \(\forall i, \lambda^*(a)\pi^* =0\)<br /> \(\forall i, r(a)-\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (\pi^*_q(a))^{q-1}+\lambda^*(a)=0\)</p><p>마지막 condition은 아래 식으로부터 나온다. \(\frac{\partial}{\partial \pi}(\underset{a \sim \pi}{E}[R] + S_q(\pi) + \mu^*(1-\underset{a}{\sum} \pi(a))+\lambda^*(a)\pi(a))\)</p><p>마지막 condition은 아래와 같이 변환이 가능하다. \(0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (\pi^*_q(a))^{q-1} + \lambda^*(a)\)<br /> \(0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\frac{(\pi^*_q(a))^{q-1}}{q-1} + \lambda^*(a)\)<br /> \(0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\frac{(\pi^*_q(a))^{q-1}+1-1}{q-1} + \lambda^*(a)\)<br /> \(0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\frac{(\pi^*_q(a))^{q-1}-1}{q-1} -1 + \lambda^*(a)\)<br /> \(0=r(a) -\mu^* - \operatorname{ln}_q(\pi^*_q(a)) - (q-1)\operatorname{ln}_q(\pi^*(a)) -1 + \lambda^*(a)\)<br /> \(0=r(a) -\mu^* - q\operatorname{ln}_q(\pi^*_q(a)) -1 + \lambda^*(a)\)</p><p>\(\pi^*_q(a) \ge 0\)이므로 \(\lambda^*(a) =0\)이 된다. 그러므로 위 공식은 아래와 같이 표현된다.</p><p>\(0=r(a) -\mu^* - q\operatorname{ln}_q(\pi^*_q(a)) -1\)<br /> \(q\operatorname{ln}_q(\pi^*_q(a))=r(a) -\mu^* -1\)<br /> \(\operatorname{ln}_q(\pi^*_q(a))=\frac{r(a)}{q} -\frac{\mu^* +1}{q}\)<br /> \(\pi^*_q(a) = \operatorname{exp}_q(\frac{r(a)}{q} - \frac{\mu^*+1}{q})\)</p><p>이제 \(\mu^*\)를 구해야한다. 이 값은 \(\underset{a}{\sum}\pi^*_q(a)\)를 이용해 구할 수 있다. \(\underset{a}{\sum}\pi^*_q(a) = \underset{a}{\sum} \operatorname{exp}_q (\frac{r(a)}{q} - \frac{\mu^*+1}{q})=1\)</p><p>이전에 \(\underset{a}{\sum} \operatorname{exp}_q (\frac{r(a)}{q} - \psi_q (\frac{r}{q}))=1\)로 표현하였다. 위 두 식이 동일하다고 하면</p>\[\psi(\frac{r}{q}) = \frac{\mu^*+1}{q}\]<p>이 된다. 이 식은 \(\mu^*\)에 대해서 풀면 해당 값을 얻을 수 있다.</p>\[\mu^* = q\psi_q(\frac{r}{q})-1\]<p>\(\pi^*_q(a) \ge 0\)를 만족해야한다.(KKT condition)</p>\[1+(q-1)(\frac{r(a)}{q} - \psi(\frac{r}{q})) \ge 0\]<p>이므로 \(\pi^*_q(a) \ge 0\)를 만족한다.(위 식은 \(\pi^*_q(a)\)의 definition에 의한 식이다.)</p><ul><li>\(\mu^*\)가 이전 공식에서 비교해서 가져오는 것이 아닌 논문 저자의 수식 표현으로 정의할 때어디서 이 표현 수식의 idea를 가져왔는 지 과정이 궁금하다.</ul><p>Optimal value problem은 아래의 수식으로 표현 가능하다.</p><p>\(\underset{a \sim \pi^*_q}{E}[R] + S_q(\pi^*_q)\)<br /> \(=\underset{a}{\sum}r(a)\pi^*_q(a) - \underset{a}{\sum}[\frac{r(a)}{q} - \psi(\frac{r}{q})]\pi^*_q(a)\)<br /> \(=\underset{a}{\sum}[r(a)\pi^*_q(a) - \frac{r(a)}{q}\pi^*_q(a) + \underset{a}{\sum}\psi_q(\frac{r}{q})\pi^*_q(a)]\)<br /> \(=\underset{a}{\sum} \frac{qr(a)-r(a)}{q} \pi^*_q(a) + \psi_q(\frac{r}{q})\underset{a}{\sum}\pi^*_q(a)\)<br /> \(=(q-1)\underset{a}{\sum}\frac{r(a)}{q}\pi^*_q(a) + \psi_q(\frac{r}{q})\)<br /> \(=(q-1)\underset{a}{\sum}\frac{r(a)}{q}\operatorname{exp}_q(\frac{r(a)}{q} - \psi_q(\frac{r}{q})) + \psi_q(\frac{r}{q})\)</p></div></details><h2 id="proposition-4">Proposition 4</h2><details> <summary> q에 따른 policy, q-potential 값 구하기</summary><div><p>When q=1 and q=2, \(\psi_1, \pi^*_1\) and \(\psi_2, \pi^*_2\)의 연산 결과는 아래와 같다.</p>\[\pi^*_1 = \operatorname{exp}(r(a)-\psi_1(r))\] \[\psi_1(r) = \operatorname{log}\underset{a}{\sum}\operatorname{exp}(r(a))\] \[\pi^*_2 = [1+ (\frac{r(a)}{2} - \psi_2(\frac{r}{2}))]_+\] \[\psi_1(\frac{r}{2}) = 1+ \frac{\underset{a \in S}{\sum}r(a)/2 -1}{\vert S \vert}\]<p>proof.</p><p>이전 공식 \(\pi^*_q(a) = \underset{a}{\sum} \operatorname{exp}(\frac{r(a)}{q} - \frac{\mu^*+1}{q})\)를 이용</p><p>q=1일 경우</p><p>\(\pi^*_1\)는 \(\operatorname{exp}(r(a) -\psi_1(r))\)이 된다.(이전의 \(\pi^*_q\)의 공식을 이용)</p>\[\underset{a}{\sum} \operatorname{exp}(r(a) - \mu^*-1)=1\] \[\underset{a}{\sum} \operatorname{exp}(r(a)) \operatorname{exp}(- \mu^*-1)=1\] \[\underset{a}{\sum} \operatorname{exp}(r(a)) = \operatorname{exp}(\mu^*+1)\] \[\operatorname{log}(\underset{a}{\sum} \operatorname{exp}(r(a))) = \mu^* +1\]<p>\(\mu^* +1 = \psi_1(r(a))\)이므로</p>\[\psi_1(r) = \operatorname{log} \underset{a}{\sum} \operatorname{exp}(r(a))\]<p>q=2일 경우</p><p>\(\pi^*_2\)는 \(\operatorname{exp}(\frac{r(a)}{2} -\psi_2(\frac{r}{2}))\)이 된다.</p>\[\underset{a \in S}{\sum} \operatorname{exp}_2(\frac{r(a)}{2} - \frac{\mu^*+1}{2}) = \underset{a \in S}{\sum}(1+\frac{r(a)}{2} - \frac{\mu^*+1}{2})=1\] \[\underset{a \in S}{\sum} (1 + \frac{r(a)}{2}) = \underset{a \in S}{\sum} (\frac{\mu^*+1}{2}) +1\] \[\underset{a \in S}{\sum} (1 + \frac{r(a)}{2}) -1 = \vert S \vert (\frac{\mu^*+1}{2})\] \[\frac{\underset{a \in S}{\sum} (1 + \frac{r(a)}{2})-1}{\vert S \vert} = \frac{\mu^* +1}{2}\] \[\frac{\vert S \vert + \underset{a \in S}{\sum} (\frac{r(a)}{2})-1}{\vert S \vert} = \frac{\mu^* +1}{2}\]<p>\(\frac{\mu^* +1}{2} = \psi_2(\frac{r}{2})\)이므로</p>\[\psi_2(\frac{r}{2})=1+ \frac{\underset{a \in S}{\sum}r(a)/2 -1}{\vert S \vert}\]</div></details><h2 id="q-maximum">q-Maximum</h2><p>MAB문제에서 MDP문제로 확장하기 전에 q-maximum에 대해서 정의를 한다. q-mximum은 maximum operator의 bounded approximation으로 아래와 같이 정의된다.</p>\[\underset{x}{\text{q-max}}(f(x)) \triangleq \underset{P \in \triangle}{\text{max}} [\underset{X \sim P} {E} [f(X)] + S-q(P)]\]<p>q-maximum operator는 아래와 같은 boundary를 가진다.</p>\[\underset{x}{\text{q-max}} (f(x)) + \text{ln}_q(1/{\vert X \vert}) \leq \underset{x}{\text{max}} (f(x)) \leq \underset{x}{\text{q-max}} (f(x))\]<p>이 식은 maximum Tsallis entropy의 MDP performance bound를 분석할 때 사용할 수 있으며 Tsallis MDP의 optimality의 조건으로 사용할 수 있다.</p><h3 id="theorem-1">Theorem 1</h3><details> <summary> q-maximum bounds 증명 </summary><div> \[\underset{x}{\text{q-max}} (f(x)) + \text{ln}_q(1/{\vert X \vert}) \leq \underset{x}{\text{max}} (f(x)) \leq \underset{x}{\text{q-max}} (f(x))\]<p>proof 1. lower bound</p><p>\(\underset{x}{\text{q-max}} (f(x)) = \underset{P \in \triangle}{\text{max}} [\underset{X \sim P}{E}[f(X)] + S_q(P)] \leq \underset{P \in \triangle}{\text{max}} \underset{X \sim P}{E}[f(X)] + \underset{P \in \triangle}{\text{max}} S_q(P)\)<br /> \(= \underset{x}{\text{max}}(f(x)) - \text{ln}_q(\frac{1}{\vert X \vert})\)</p>\[\underset{x}{\text{q-max}}(f(x)) + \text{ln}_q(1/{\vert X \vert}) \leq \underset{x}{\text{max}}(f(x))\]<p>proof 2. upper bound</p><p>P’이 한 값만 뽑을 수 있게 되어 있다면(P’(x) =1 when \(x=\text{argmax}_{x'}f(x')\), others P(x’)=0) P’의 Tsallis entropy는 0이 될 것이다.</p>\[\underset{x}{\text{q-max}}(f(x)) = \underset{P \in \triangle}{\text{max}} [\underset{X \sim P}{E}[f(X)] + S_q(P)] \geq \underset{X \sim P'}{E} [f(x)] + S_q(P') = f(\underset{x'}{\text{argmax}}f(x')) = \underset{x}{\text{max}}f(x)\] \[\underset{x}{\text{q-max}}(f(x)) \geq \underset{x}{\text{max}}f(x)\]</div></details><h1 id="4-maximum-tsallis-entropy-in-mdps">4. Maximum Tsallis Entropy in MDPs</h1><p>이젠 MAB가 아닌 Tsallis entropy를 최대화하는 MDP를 정의한다. Tsaalis entropy는</p>\[S^\infty_q (\pi) \triangleq E_{\tau \sim P, \pi}[\sum^\infty_{t=0} \gamma^t S_q(\pi(\cdot | s_t))]\]<p>로 정의된다. 이를 원래 MDP의 최대화 공식에 넣으면 아래와 같이된다.</p>\[\underset{\pi \in \Pi}{\text{maximize}} [\sum^\infty_t \gamma^t R_t] + \alpha S^\infty_q(\pi)\]<p>이 식을 통하여 state value와 state action value function은 아래와 같이 정의된다.</p>\[V^\pi_q(s) \triangleq E_{\tau \sim P, \pi} [\sum^\infty_{t=0} \gamma^t (R_t + \alpha S_q(\pi(\cdot|s_t)))|s_0=s]\] \[Q^\pi_q(s,a) \triangleq E_{\tau \sim P, \pi} [R_0 + \gamma V^\pi_q(s_1) |s_0=s, a_0=a]\]<p>Tsallis MDP의 목적은 optimal policy distribution을 찾는 것이다. 우선 \(\pi\)를 \(\rho\)로 표현할 것이며 이 식이 \(\rho\)에 대해서 concave한 것을 보여줄 것이다. 마지막으로 error bound의 크기를 정의할 것이다.</p><h2 id="proposition-56-and-lemma-91">Proposition 5,6 and lemma 9.1</h2><details> <summary> TBE 증명하기 전 proposition 5,6, lemma 9.1 </summary><div><p>Theorem 9를 증명하기 전 2가지 proposition과 한 개의 lemma를 먼저 얘기하고자한다. 이 식들은 \(\pi\)를 state action visitation인 \(\rho\)로 변환하는 데 사용된다.</p><ul><li>Proposition 5.</ul>\[\rho_\pi(s) = E_{\tau \sim P, \pi}[\sum^\infty_{t=0} \gamma^t 1(s_t=s)]\] \[\rho_\pi(s) = E_{\tau \sim P, \pi}[\sum^\infty_{t=0} \gamma^t 1(s_t=s,a_t=a)]\]<p>위 두식을 만족한다면 식을 만족한다.</p>\[\rho_\pi(s) = \sum_a \rho_\pi(s,a)\] \[\rho_\pi(s,a) = \rho_\pi(s) \pi(a|s)\] \[\sum_a\rho_\pi(s,a) = d(s) + \sum_{s',a'} P(s|s',a') \rho_\pi(s',a')\]<ul><li>Proposition 6.</ul>\[M \triangleq { \rho| \forall s, a, \rho(s,a) \geq 0, \sum_a \rho(s,a) = d(s) + \sum_{s',a'}P(s|s',a') \rho(s',a')}\]<p>만약 \(\rho \in M\)이라면 state action visitation을 이용해 \(\pi_\rho(a|s) \triangleq \frac{\rho(s,a)}{\sum_{a'}\rho(s,a')}\) 로 표현할 수 있고 \(\pi_\rho\)는 state-action visitation이 \(\rho\)인 유일한 policy이다.</p><ul><li>Lemma 9.1</ul><p>위 두 Proposition 5,6을 통하여 policy와 state action visitation이 1:1 대응이 된다는 것을 알 수 있었다. 이 lemma에서는 \(\pi\)를 \(\rho\)로 변환할 것이다.</p>\[\bar{S}^\infty_q(\rho) = - \sum_{s,a} \rho(s,a) \text{ln}_q(\frac{\rho(s,a)}{\sum_{a'}\rho(s,a')})\]<p>위 식을 만족한다면 \(\pi \in \Pi\)의 어떤 stationary policy와 \(\rho \in M\)의 어떤 state-action visitation measure에 대해서 \(S^\infty_q(\pi) = \bar{S}^\infty_q(\rho_\pi)\)를 만족한다.</p><p>Proof.</p>\[S^\infty_q(\pi) = E_{\tau \sim P, \pi}[\sum^\infty_{t=0} \gamma^t S_q(\pi(\cdot|s_t))] = \sum_s S_q(\pi(\cdot|s)) \cdot E_{\tau \sim P, \pi} [\sum^\infty_{t=0} \gamma^t 1(s_t=s)]\] \[= \sum_s S_q(\pi(\cdot | s)) \rho_\pi(s) = \sum_{s,a} - \text{ln}_q(\pi(a|s)) \pi(a|s) \rho_\pi(s)\] \[= \sum_{s,a} -\text{ln}_q(\pi(a|s)) \frac{\rho(s,a)}{\sum_{a'}\rho(s,a')} \sum_{a'} \rho(s,a')\] \[=\sum_{s,a} - \text{ln}_q(\frac{\rho_\pi(s,a)}{\sum_{a'} \rho_\pi(s,a')})\rho_\pi(s,a)=\bar{S}^\infty_q(\rho_\pi)\]<ul><li>Corollary 9.1.1</ul><p>\(\rho\)가 optimal이면 \(\pi\)가 optimal인 것을 설명</p>\[J(\pi') + S^\infty_q(\pi') &gt; J(\pi_{\rho^*}) + S^\infty_q(\pi_{\rho^*})\]<p>where, \(J(\pi) = E_{\tau \sim \pi, P} [\sum^\infty_{t=0} \gamma^t R_t]\)</p><p>위 식은 아래 식으로 변경이 가능하다.</p>\[\sum_{s,a} \rho_{\pi'}(s,a) r(s,a) + \bar{s}^\infty_q(\rho_{\pi'}) &gt; \sum_{s,a} \rho^*(s,a) r(s,a) + \bar{s}^\infty_q(\rho^*)\]<p>하지만 \(\rho^*\)는 optimal이기 때문에 모순이다.</p>\[J(\pi') + S^\infty_q(\pi') \leq J(\pi_{\rho^*}) + S^\infty_q(\pi_{\rho^*})\]<p>위 식이 맞는 식이 되고 \(\rho^*\)가 optimal이면 \(\pi^*\)도 optimal이 된다.</p></div></details><h2 id="lemma-92">Lemma 9.2</h2><details> <summary> Variable Change</summary><div> \[\underset{\rho}{\text{maximize}} \sum_{s,a} \rho(s,a) \sum_{s'} r(s,a,s')P(s'|s,a) - \sum_{s,a} \rho(s,a) \text{ln}_q (\frac{\rho(s,a)}{\sum_{a'}\rho(s,a')})\]<p>subject to \(\forall s, a, \rho(s,a) \geq 0, \sum_a \rho(s,a) = d(s) + \sum_{s',a'} P(s |s',a') \rho(s',a')\)</p><p>이전의 Proposition 6과 Lemma 9.1을 통하여 Tsallis MDP를 \(\pi\) 대신 \(\rho\)로 표현이 가능하다. 그리고 이 ㅅ식을 이용해서 convcave한 것을 증명할 것이다.</p><p>Lemma 9.2 \(\bar{S}^\infty_q(\rho)\) is concave function with respect to \(\rho \in M\)</p><p>Proof.</p><p>\(f(x) = -x\text{ln}_q(x)\)라고 할 때 2차 미분 값은 \(-qx^{q-2}\)가 되고 이 값은 x가 0보다 클 때 음수가 된다. 즉 f(x)는concave function이라고 할 수 있다. concave function은 다른 방법으로 아래 식과 같은 중간값 정리를 이용하여 증명할 수 있다.</p>\[\bar{S}^\infty_q(\lambda_q \rho_1 + \lambda_2 \rho_2) &gt; \lambda_1 \bar{S}^\infty_q(\rho_1) + \lambda_2 \bar{S}^\infty_q (\rho_2)\]<p>where, \(0&lt;\lambda_1, \lambda_2 &lt; 1\), \(\lambda_1 + \lambda_2 =1\), \(\rho_1, \rho_2 \in M\)</p><p>\(\widetilde{\rho}\)를 \(\lambda_1 \rho_1 + \lambda_2 \rho_2\)라고 정의하고 \(\mu_1 = \frac{\lambda_1 \sum_{a'} \rho_1(s,a')}{\sum_{a'} \widetilde{\rho}(s,a')}\) 그리고 \(\mu_2 = \frac{\lambda_2 \sum_{a'} \rho_2(s,a')}{\sum_{a'} \widetilde{\rho}(s,a')}\) 라고 정의한다. 그리고 \(\mu_1 + \mu_2 =1\)이 된다.</p>\[\bar{S}^\infty_q(\lambda_1 \rho_1 + \lambda_2 \rho_2) = -\sum_{s,a} \widetilde{\rho}(s,a) \text{ln}_q (\frac{\lambda_1 \rho_1 (s,a) + \lambda_2 \rho_2(s,a)}{\sum_{a'} \widetilde{\rho}(s,a')})\] \[= -\sum_{s,a} \widetilde{\rho}(s,a) \text{ln}_q (\frac{\lambda_1 \rho_1 (s,a)}{\sum_{a'} \widetilde{\rho}(s,a')}+ \frac{\lambda_2 \rho_2(s,a)}{\sum_{a'} \widetilde{\rho}(s,a')})\] \[= \sum_{s,a} \widetilde{\rho}(s,a) \text{ln}_q (\lambda_1 \frac{\sum_{a'} \rho_1(s,a')}{\sum_{a'} \rho_1 (s,a')} \frac{\rho_1(s,a)}{\sum_{a'} \widetilde{\rho}(s,a')} + \lambda_2 \frac{\sum_{a'} \rho_2(s,a')}{\sum_{a'} \rho_2 (s,a')} \frac{\rho_2(s,a)}{\sum_{a'} \widetilde{\rho}(s,a')} )\] \[= - \sum_{s,a} \widetilde{\rho}(s,a) \text{ln}_q (\mu_1 \frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')} + \mu_2 \frac{\rho_2(s,a)}{\sum_{a'} \rho_2(s,a')})\] \[= - \sum_{s,a} \frac{\sum_{a'}\widetilde{\rho}(s,a')}{\sum_{a'}\widetilde{\rho}(s,a')}(\lambda_1 \rho_1(s,a)+\lambda_2 \rho_2(s,a)) \text{ln}_q (\mu_1 \frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')} + \mu_2 \frac{\rho_2(s,a)}{\sum_{a'} \rho_2(s,a')})\] \[= - \sum_{s,a} \sum_{a'} \widetilde{\rho}(s,a') (\frac{\mu_1 \rho_1(s,a)}{\sum_{a'} \rho_1(s,a')} + \frac{\mu_2 \rho_2(s,a)}{\sum_{a'} \rho_2(s,a')}) \text{ln}_q (\frac{\mu_1 \rho_1(s,a)}{\sum_{a'} \rho_1 (s,a')} + \frac{\mu_2 \rho_2(s,a)}{\sum_{a'} \rho_2 (s,a')})\]<p>그러므로 아래 식을 만족한다.</p><p>\(- (\frac{\mu_1 \rho_1(s,a)}{\sum_{a'} \rho_1(s,a')} + \frac{\mu_2 \rho_2(s,a)}{\sum_{a'} \rho_2(s,a')}) \text{ln}_q (\frac{\mu_1 \rho_1(s,a)}{\sum_{a'} \rho_1 (s,a')} + \frac{\mu_2 \rho_2(s,a)}{\sum_{a'} \rho_2 (s,a')})\) \(&gt; - \mu_1 \frac{\rho_1(s,a)}{\sum_{a'}\rho_1(s,a')} \text{ln}_q (\frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')}) - \mu_2 \frac{\rho_2(s,a)}{\sum_{a'}\rho_2(s,a')} \text{ln}_q (\frac{\rho_2(s,a)}{\sum_{a'} \rho_2(s,a')})\)</p><p>\(\sum_{a'} \widetilde{\rho}(s,a') \mu_1 \frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')}= \sum_{a'} \widetilde{\rho}(s,a') \frac{\lambda_1 \sum_{a'} \rho_1 (s,a')}{\sum_{a'} \widetilde{\rho}(s,a')} \frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')} = \lambda_1\rho_1(s,a)\)이므로 우리는 위 식을 정리하면 아래를 얻을 수 있다.</p>\[\bar{S}^\infty_q (\lambda_1 \rho_1 + \lambda_2 \rho_2) &gt; - \sum_{s,a} \sum_{a'} \widetilde{\rho}(s,a') \mu_1 \frac{\rho_1(s,a)}{\sum_{a'}\rho_1(s,a')} \text{ln}_q(\frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')})\] \[\qquad\qquad\qquad\qquad- \sum_{s,a} \sum_{a'} \widetilde{\rho}(s,a') \mu_2 \frac{\rho_2(s,a)}{\sum_{a'}\rho_2(s,a')} \text{ln}_q(\frac{\rho_2(s,a)}{\sum_{a'} \rho_2(s,a')})\] \[= - \sum_{s,a} \lambda_1 \rho_1(s,a) \text{ln}_q (\frac{\rho_1(s,a)}{\sum_{a'} \rho_1(s,a')}) - \sum_{s,a} \lambda_2 \rho_2(s,a) \text{ln}_q (\frac{\rho_2(s,a)}{\sum_{a'}\rho_2(s,a')})\] \[= \lambda_1 \bar{S}^\infty_q (\rho_1) + \lambda_2 \bar{S}^\infty_q(\rho_2)\]<p>위 식을 통해 concave함을 증명하였다. (q가 0보다 클 경우)</p><ul><li>Corollary 9.2.1</ul>\[\underset{\rho}{\text{maximize}} \sum_{s,a} \rho(s,a) \sum_{s'} r(s,a,s')P(s'|s,a) - \sum_{s,a} \rho(s,a) \text{ln}_q (\frac{\rho(s,a)}{\sum_{a'}\rho(s,a')})\]<p>subject to \(\forall s, a, \rho(s,a) \geq 0, \sum_a \rho(s,a) = d(s) + \sum_{s',a'} P(s |s',a') \rho(s',a')\)</p><p>이 식은 첫 번째 term은 linear하고 두 번째 term은 concave하니 concave problem이라고 할 수 있다.</p></div></details><h2 id="tsallis-bellman-optimality-equation">Tsallis Bellman Optimality Equation</h2><p>q-maximum operator를 이용하여 Tsallis MDP의 optimality condition은 아래와 같이 정의할 수 있다.</p>\[Q^*_q(s,a) = E_{s' \sim P} [r(s,a,s') + \gamma V^*_q(s') |s,a]\] \[V^*_q(s) = \underset{a}{\text{q-max}} (Q^*_q(s,a))\] \[\pi^*_q(a|s) = \text{exp}_q (Q^*_q(s,a)/q - \psi_q(Q^*_q(s,\cdot)/q))\]<p>where, \(\psi_q\) is a q-potential function.</p><p>위 식을 얻기 위해서 이전과 다르게 KKT condition을 이용해서 구했다.</p><h3 id="theorem-9">Theorem 9</h3><details> <summary> Proof of Tsallis Bellman Optimality Equation</summary><div><p>\(L \triangleq \sum_{s,a} \rho(s,a) r(s,a) - \sum_{s,a} \rho(s,a) \text{ln}_q (\frac{\rho(s,a)}{\sum_{a'}\rho(s,a')}) + \sum_{s,a} \lambda (s,a) \rho(s,a)\) \(+ \sum_s \mu(s) (d(s) + \sum_{s',a'} P(s|s',a') \rho(s',a') + \sum_a \rho(s,a))\)</p><p>where \(\lambda(s,a)\) and \(\mu(s)\) are dual variables for nonnegativity and Bellman flow constraints.</p><p>KKT condition:</p>\[\forall s, a, \rho^*(s,a) \geq 0, d(s) + \sum_{s',a'}P(s|s',a') \rho^*(s'a') - \sum_a \rho^*(s,a) =0\] \[\forall s, a, \lambda^*(s,a) \leq 0\] \[\forall s,a, \lambda^*(s,a) \rho^*(s,a) =0\] \[\forall s,a, 0 = \sum_{s'}r(s,a,s')P(s'|s,a) + \gamma \sum_{s'} \mu^*(s') P(s'|s,a) - \mu^*(s)\] \[- q\text{ln}_q(\frac{\rho^*(s,a)}{\sum_{a'}\rho^*(s,a')})-1 + \sum_a (\frac{\rho^*(s,a)}{\sum_{a'}\rho^*(s,a')}) + \lambda^*(s,a)\]<p>Lagrangian objective에서 \(\bar{S}^\infty_q(\rho)\)의 derivation된 kkt condition 값은 아래와 같이 유도된다.</p><p>\(\frac{\partial \bar{S}^\infty_q(\rho)}{\partial \rho(s'',a'')} = -\sum_{s,a} \frac{\partial \rho(s,a)}{\partial \rho(s'',a'')} \text{ln}_q (\frac{\rho(s,a)}{\sum_{a'} \rho(s,a')})\) \(- \sum_{s,a} \rho(s,a) \frac{\partial \text{ln}_q(\rho(s,a)/\sum_{a'} \rho(s,a'))}{\partial \rho(s'',a'')} -1\)</p><p>\(= -\text{ln}_q(\frac{\rho(s'',a'')}{\sum_{a'}\rho(s''a,')})\) \(- \sum_a \rho(s'',a)(\frac{\rho(s'',a)}{\sum_{a'}\rho(s'',a')})^{q-2}(\frac{\delta_{a''}(a)}{\sum_{a'}\rho(s'',a')} - \frac{\rho(s'',a)}{(\sum_{a'} \rho(s'',a'))^2})\)</p>\[= -\text{ln}_q(\frac{\rho(s'',a'')}{\sum_{a'}\rho(s'',a')}) - (\frac{\rho(s'',a'')}{\sum_{a'}\rho(s'',a')})^{q-1} + \sum_a(\frac{\rho(s'',a)}{\sum_{a'}\rho(s'',a')})^q\] \[= -q\text{ln}_q(\frac{\rho(s'',a'')}{\sum_{a'}\rho(s'',a')}) - 1 + \sum_a(\frac{\rho(s'',a)}{\sum_{a'}\rho(s'',a')})^q\]<ul><li>\(\text{ln}_q x + x^{q-1} = \frac{x^{q-1}-1}{q-1}+x^{q-1} = \frac{x^{q-1}-1 + (q-1)x^{q-1}}{q-1} = \frac{qx^{q-1}-1}{q-1}\) \(= \frac{qx^{q-1}-1 +q-q}{q-1} = q\frac{x^{q-1}-1}{q-1} +1 = q\text{ln}_qx +1\)</ul><p>\(\mu^*(s)\)를 optiaml value인 \(V^*_q(s)\)라고 칭하고 싶다. 그래서 stationary condition에 \(\pi_{\rho^*}(a|s) = \rho^*(s,a)/\sum_{a'} \rho^*(s,a')\)를 곱한다. 그리고 a에 대해서 정리를 하면 아래처럼 정리된다.</p>\[0=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s) - \mu^*(s)\] \[-q\sum_a \pi_{\rho^*}(a|s)\text{ln}_q(\frac{\rho^*(s,a)}{\sum_{a'}\rho^*(s,a')}) -1+ \sum_a \pi_{\rho^*}(a|s) \sum_{a''}(\frac{\rho^*(s,a'')}{\sum_{a'}\rho^*(s,a')})^q\] \[+ \sum\lambda^*(s,a)\pi_{\rho^*}(a|s)\] \[=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s) - \mu^*(s)\] \[-q\sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s)) -1+ \sum_{a''}\pi_{\rho^*}(s,a)^q + \sum_a \lambda^*(s,a) \pi_{\rho^*}(a|s)\]<p>KKT condition에 의해서 \(\lambda^*(s,a) =0\)</p>\[=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s) - \mu^*(s)\] \[-q\sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s)) -1+ \sum_{a''}\pi_{\rho^*}(s,a)^q\] \[=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s) - \mu^*(s)\] \[-q\sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s)) -1+ \sum_{a''}\pi_{\rho^*}(s,a)^q\]<p>\(\sum_{s,a}x^q -1 = \sum_{s,a}(x^q-x) = \sum{s,a} (q-1) \frac{x^{q-1}-1}{q-1}x = (q-1)\sum_{s,a}x\text{ln}_qx\) 식을 이용</p>\[=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s) - \mu^*(s)\] \[-q\sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s)) + (q-1)\sum_{s,a} \pi_{\rho^*}(s,a) \text{ln}_q(\pi_{\rho^*}(s,a))\] \[=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s) - \mu^*(s)\] \[-\sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s))\]<p>그러므로</p>\[\mu^*(s)=\sum_a\sum_{s'} r(s,a,s') P(s'|s,a)\pi_{\rho^*}(a|s) + \gamma \sum_{s'} \mu^*(s') \sum_a P(s'|s,a) \pi_{\rho^*}(a|s)\] \[-\sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s))\] \[=E_{s' \sim P,a \sim \pi}[r(s,a,s') + \alpha S_q(\pi_{\rho^*}(\cdot | s)) + \gamma\mu^*(s') |s]\]<p>위 식은 Tsallis Bellman expectation(TBE)를 만족한다. 그러므로 \(\mu^*(s)\)를 $$V^{\pi_{\rho^*}}인 value of optimal policy라고 칭할 것이다. 물론 추가적인 증명이 필요하지만 이는 Theorem 10에서 증명을 할 것이다.</p><p>\(\mu^*=V^*_q\)라면 \(Q^*_q(s,a) = E_{s' \sim P}[r(s,a,s') + \gamma\mu^*(s')]\)를 얻을 수 있다.</p><p>\(\rho^*(s,a)/\sum_{a'}\rho^*(s,a')\) 를 \(\pi_{\rho^*}(a|s)\)로 치환하고, \(Q^*_q(s,a) = E_{s' \sim P} [r(s,a,s') + \gamma \mu^*(s')]\), \(\mu^*(s) = V^*(s)\)라고 한다면 아래 식을 KKT condition을 이용하여 구할 수 있다.</p>\[Q^*_q(s,a) - V^*_q(s) - q\text{ln}_q(\pi_{\rho^*}(a|s)) -1 + \sum_a \pi_{\rho^*}(a|s)^q = 0\] \[\frac{Q^*_q(s,a)}{q} - \frac{V^*_q(s) +1 - \sum_a \pi_{\rho^*}(a|s)^q}{q} = \text{ln}_q(\pi_{\rho^*}(a|s))\] \[\text{exp}_q{\frac{Q^*_q(s,a)}{q} - \frac{V^*_q(s) +1 - \sum_a \pi_{\rho^*}(a|s)^q}{q}} =\pi_{\rho^*}(a|s)\]<p>\(\sum_a\pi(a|s) =1\) 이므로 양변에 \(\sum\)을 넣는다.</p>\[\sum_a(\text{exp}_q{\frac{Q^*_q(s,a)}{q} - \frac{V^*_q(s) +1 - \sum_a \pi_{\rho^*}(a|s)^q}{q}}) =1\]<p>이 식은 q-exponential distribution의 normalization equation이다. 그러므로 q-potential과 optimal value function과 관계를 얻을 수 있다.(식 24를 참고)</p>\[\psi_q(\frac{Q^*_q(s, \cdot)}{q}) = \frac{V^*_q(s) +1 - \sum_a(\pi_{\rho^*}(a|s))^q}{q}\]<p>결론적으로, optimal policy는 \(Q^*_q(s,\cdot)\) q-exponential distribution을 얻을 수 있다.</p>\[\text{exp}_q(\frac{Q^*_q(s,a)}{q} - \psi_q(\frac{Q^*_q(s,\cdot)}{q})) = \pi_{\rho^*}(a|s)\]<p>이를 \(\mu^*\) 식에 대입하면 아래 식을 얻을 수 있다.</p>\[V^*_q(s) = \sum_a \pi_{\rho^*}(a|s) \sum_{s'} [r(s,a,s') + \gamma V^*_q(s') P(s'|s,a)] - \sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s))\] \[= \sum_a \pi_{\rho^*}(a|s) Q^*_q(s,a) - \sum_a \pi_{\rho^*}(a|s) \text{ln}_q(\pi_{\rho^*}(a|s))\] \[=\sum_a \pi_{\rho^*}(a|s) Q^*_q(s,a) - \sum_a \pi_{\rho^*}(a|s)(\frac{Q^*_q(s,a)}{q} - \psi_q(\frac{Q^*_q(s,\cdot)}{q}))\] \[=(q-1)\sum_a \pi_{\rho^*}(a|s)\frac{Q^*_q(s,a)}{q} + \psi_q(\frac{Q^*_q(s,\cdot)}{q})\] \[=\underset{a'}{q-\text{max}}(Q^*_q(s,a'))\]<p>결론으로 아래의 Tsallis MDP의optimality condition를 얻을 수 있다.</p>\[Q^*_q(s,a) = E_{s' \sim P} [r(s,a,s') + \gamma V^*_q(s') |s,a]\] \[V^*_q(s) = \underset{a}{\text{q-max}} (Q^*_q(s,a))\] \[\pi^*_q(a|s) = \text{exp}_q (Q^*_q(s,a)/q - \psi_q(Q^*_q(s,\cdot)/q))\]</div></details><h1 id="5-dynamic-programming-for-tsallis-mdps">5. Dynamic Programming for Tsallis MDPs</h1><p>Tsallis policy tieration(TPI)와 Tsallis value iteration(TVI)라는 Tsallis MDP를 위한 dynamic programming algorithm을 소개한다. TPI는 policy evaluation과 improvement로 구성되어 있는 policy iteration method이다. fixed policy의 value function이 연산되고 난 후 policy가 value function에 의해 업데이트된다. TVI는 optimal value가 바로 업데이트되는 방식이다.</p><h2 id="tsallis-policy-iteration">Tsallis Policy Iteration</h2><p>Fixed plicy \(\pi\)를 이용하여 \(V^\pi_q\)와 \(Q^\pi_q\)를 연산한다. 원래 MDP와 비슷하게 Tsallis MDP는 expectation equation을 이용하여 연산할 수 있다.</p>\[Q^\pi_q(s,a) = E_{s' \sim P} [r(s,a,s') + \gamma V^\pi_q(s') |s,a]\] \[V^\pi_q(s) = E_{a \sim \pi}[Q^\pi_q(s,a) +\text{ln}_q(\pi(a|s))]\]<p>위 식은 Tsallis Bellman Expectation(TBE)라고 칭한다.</p><h3 id="theorem-10">Theorem 10</h3><details> <summary> Tsallis Policy Evaluation</summary><div> \[[\Gamma^\pi_qF] (s,a) \triangleq E_{s' \sim P} [r(s,a,s') + \gamma V_F(s') |s,a]\] \[V_F(s) \triangleq E_{a \sim \pi} [F(s,a) - \text{ln}_q(\pi(a|s))]\]<ul><li>Lemma 10.1</ul>\[\Gamma^\pi_q(F + c1) = \Gamma^\pi_qF + \gamma c1\]<p>Proof.</p>\[V_{F+c1}(s) = E_{a \sim \pi} [F(s,a) + c - \text{ln}_q(\pi(a|s))] = E_{a \sim \pi}[F(s,a) - \text{ln}_q(\pi(a|s))]+c= V_F(s) + c\] \[[\Gamma^\pi_q(F+c1)](s,a) = E_{s' \sim P} [r(s,a,s') + \gamma V_{F+c1}(s')|s,a] = E_{s' \sim P} [r(s,a,s') +\gamma V_F(s') + \gamma c |s,a]\] \[= E_{s' \sim P} [r(s,a,s') + \gamma V_F(s') |s,a] + \gamma c = \Gamma^\pi_q F(s) + \gamma c\]<ul><li>Lemma 10.2</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/lemma10_2.png" alt="lemma10.2" /></p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>* 위 식의 부등호 방향이 맞는 지 의문이 있긴함.
</pre></table></code></div></div><ul><li>Lemma 10.3</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/lemma10_3.png" alt="lemma10.3" /></p><p>Fixed policy \(\pi\)와 entropic index \(q \ge 1\), TBE operator \(\Gamma^\pi_q\)</p><p>Tsallis policy evaluation은 \(F_{k+1} = \Gamma^\pi_qF_k\)로 정의할 수 있다. 그러면 \(F_k\)가 converge하다면 TBE equation을 만족한다.</p><p>Proof of Tsallis Policy Evaluation</p><p>위의 lemma를 통해 \(\Gamma^\pi_q\)가 \(\gamma\)-contraction임을 증명하였고 Banach fixed point theorem에 의해 unique fixed point \(F_* = \Gamma^\pi_qF_*\)임을 얻을 수 있다. 그러므로 F에서부터 \(F_k\) sequence는 fixed point로 converge하다는 것을 얻을 수 있다. \(F_*\)가 unique이므로 \(F_*\)는 TBE equation을 만족하는 유일한 function이다. 그러므로 \(F_*=Q^\pi_q\)이다.</p></div></details><h3 id="theorem-11-12">Theorem 11, 12</h3><details> <summary> Tsallis Policy Improvement, Optimality of TPI </summary><div><ul><li>Theorem 11: \(\pi_{k+1}\)을 updated policy라고 한다면 \(Q^{\pi_{k+1}}_q(s,a)\)는 \(Q^{\pi_k}_q(s,a)\)보다 더 크다.</ul><p>Proof</p><p>\(\pi_{k+1}\)은 updated policy이고 policy를 최대화하는 것은 concave하므로 아래 식을 만족한다.</p>\[E_{a \sim \pi_{k+1}} [Q^{\pi_k}_q(s,a) - \text{ln}_q(\pi_{k+1}(a|s)) |s] \ge E_{a \sim \pi_{k}} [Q^{\pi_k}_q(s,a) - \text{ln}_q(\pi_{k}(a|s)) |s] = V^{\pi_k}_q(s)\] \[Q^{\pi_k}_q(s,a) = E_{s_1 \sim P}[r(s_0,a_0,s_1) + \gamma V^{\pi_k}_q(s_1) | s_0=s, a_0=a]\] \[\le E_{s_1 \sim P}[r(s_0,a_0,s_1) |s_0=s,a_0=a] + \gamma E_{s_1,a_1 \sim P, \pi_{k+1}}[Q^{\pi_k}_q(s_1,a_1) - \text{ln}_q(\pi_{k+1}(a_1|s_1))|s_0=s,a_0=a]\] \[= E_{s_1 \sim P} [r(s_0,a_0,s_1) | s_0=s,a_0=a] + \gamma E_{s_{1:2},a_1 \sim P, \pi_{k+1}}[r(s_1,a_1,s_2) - \text{ln}_q(\pi_{k+1}(a_1|s_1)) + \gamma V^{\pi_k}_q(s_2) | s_0=s, a_0=a]\] \[\le E_{s_1 \sim P}[r(s_0,a_0,s_1) + \gamma V^{\pi_{k+1}}_q(s_1) |s_0=s, a_0=a] = Q^{\pi_{k+1}}_q(s,a)\]<ul><li>Theorem 12(Optimality of TPI): TPI converges into an optimal policy and value of a Tsallis MDP.</ul><p>현재까지 TBE와 식의 Improvement를 증명하였다. 여기서는 TPI가 optimal policy와 Tsallis MDP value로 converge함을 증명하고자 한다.</p><p>Proof.</p><p>reward function r이 \(r_{\text{max}}\)의 uppder bound이고 \(Q^{\pi_k}_q\) 또한 한계가 있다. 그리고 \(Q^{\pi_k}_q\)는 monotonically non-decreasing(Theorem 11)이므로 어떤 \(\pi_*\)에 converge한다. \(Q^{\pi_k}_q\)의 unique한 solution은 \(\pi_*\)이고 Theorem 9에 의해 식들이 Optimal하므로 \(\pi_*=\pi^*_q\)가 되어 optimal solution이 된다.</p></div></details><h2 id="tsallis-value-iteration">Tsallis Value Iteration</h2><p>Tsallis value iteration은 Tsallis policy evaluation과 동일한 방법으로 증명한다. Tsallis value iteration은 \(\Gamma j_q\)의 contraction property에 의해 converge하다. 그러므로 \(F_k\)는 fixed point \(\Gamma_q\)에 converge하다. 이 fixed point는 TBO equation을 만족한다.</p><h2 id="performance-error-bounds">Performance Error Bounds</h2><p>TPI 혹은 TVI로부터 얻은 Tsallis MDP의 optimal policy의 error bound를 설명할 것이다. 이 error는 Tsallis entropy maximization에서 사용된 regularization term에 의해 발생한다. Tsallis MDP와 original MDP를 비교해서 error performance를 비교한다.</p><h3 id="theorem-14">Theorem 14</h3><details> <summary>Performance Error Bounds </summary><div><ul><li>lemma 14.1</ul><p>\([\Gamma F](s,a)\) \(\triangeq E_{s' \sim P}[r(s,a,s') + \gamma \underset{a'}{\text{max}}F(s',a') | s,a]\)</p>\[\Gamma^k_q F \succeq \Gamma^kF\]<p>Proof. k=1이라면 Lemma 13.1에 의해 아래 식을 만족한다.</p><p>\([\Gamma F]\) \((s,a) = E_{s' \sim P} [r(s,a,s') + \gamma \underset{a'}{\text{max}}F(s',a') |s,a]\) \(\leq E_{s' \sim P}[r(s,a,s') + \gamma q-\underset{a'}{\text{max}}F(s',a') | s,a] =\) \([\Gamma_qF](s,a)\)</p><p>k=n이라면</p>\[\gamma^{n+1}F = \Gamma \Gamma^n F \preceq \Gamma_q\Gamma^nF \preceq \Gamma_q \Gamma^n_qF \preceq \Gamma^{n+1}_qF\]<p>그러므로</p>\[V^* = \underset{t \to \infty}{\text{lim}} \Gamma^kF \preceq \underset{k \to \infty}{\text{lim}} \Gamma^k_q F = V^*_q\]<p>Theorem 14.</p><p>Proof.</p><p>Upperbound는 original MDP가 \(J(\pi)\)를 최대화 하기 때문에 \(J(\pi^*_q) \leq J(\pi^*)\)가 된다.</p><p>Lower bound는 아래 식을 이용하여 증명된다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/error_bound.png" alt="error bound" /></p></div></details><p>TPI와 TVI는 transition probability P를 가지고 value function을 update한다. 게다가 \(\psi_q\)를 연산하기 힘들다는 문제가 있다. 그래서 TPI를 actor-critic method로 확장하여 이 문제를 해결하고 large-scale model-free RL문제를 해결하고자 한다.</p><h1 id="tsallis-actor-critic-for-model-free-rl">Tsallis Actor Critic for Model-Free RL</h1><p>TAC에서는 5 가지 network를 사용한다. policy \(\pi_\phi\), state value \(V_\psi\), target value \(V_{\psi^-}\), 2 action values \(Q_{\theta_1}\), \(Q_{\theta_2}\)</p><p>policy를 update하는 것에 \(\pi_\phi\)가 필요하지만 intractable하기 때문에 stochastic gradient를 사용한다. 그리고 reparameterization을 이용하여 approximation한다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/algorithm.png" alt="Algorithm" /></p><h1 id="experiment">Experiment</h1><p>이 논문에서는 MuJoCo를 이용하여 다른 논문과 비교를 하였다. 우선 entropic index가 TAC에 어떤 영향을 미치는 지 실험을 하였다.</p><p>q value를 {0.5, 0.7, 1.0, 1.2, 1.5, 1.7, 2.0, 3.0, 5.0}으로 변경하며 실험하였다. entropy coefficient \(\alpha\)는 0.05(Humanoid-v2), 0.2(otheres)로 고정하였다.</p><p>실험 결과 \(1 \le q &lt;2\) 일경우 가장 좋은 결과를 보여주었다. \(0 &lt; q &lt;1\)일 경우 별로인 결과를 보여주었다. SG entropy보다 더 exploitation을 방해하기 때문이다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/exp1.png" alt="exp1" /></p><p>다른 논문과 비교시 TAC를 제외하고 SAC가 가장 좋은 결과를 보여주었고 TAC는 SAC보다 짧은 episode안에 좋은 결과를 보여주는 것을 확인할 수 있다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/exp2.png" alt="exp2" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/Single-agent/tac/exp3.png" alt="exp3" /></p><h1 id="reference">Reference</h1><ol><li><a href="https://arxiv.org/pdf/1902.00137.pdf">Lee, Kyungjae, et al. “Tsallis reinforcement learning: A unified framework for maximum entropy reinforcement learning.” arXiv preprint arXiv:1902.00137 (2019).</a></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/single-agent/'>Single-Agent</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/single-agent/" class="post-tag no-text-decoration" >Single-Agent</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning - JG_blog&url=https://leejungi.github.io/posts/tac/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning - JG_blog&u=https://leejungi.github.io/posts/tac/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning - JG_blog&url=https://leejungi.github.io/posts/tac/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DAGMM/">(Zong 2018 ICLR) Deep Autoencoding Gaussian Mixture Model For Unsupervised Anomaly Detection</a><li><a href="/posts/HIRO/">(Ofir 2018 Nips) Data-Efficient Hierarchical Reinforcement Learning</a><li><a href="/posts/FUN/">(Vezhnevets 2017 ICML) Feudal networks for hierarchical reinforcement learning</a><li><a href="/posts/ASAC/">(Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications</a><li><a href="/posts/SAC/">(Haarnoja 2018 ICML) Soft Actor-Critic; Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/single-agent/">Single-Agent</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/multi-agent/">Multi-Agent</a> <a class="post-tag" href="/tags/hierarchial-rl/">Hierarchial RL</a> <a class="post-tag" href="/tags/anomaly-detection/">Anomaly Detection</a> <a class="post-tag" href="/tags/distributed-rl/">Distributed RL</a> <a class="post-tag" href="/tags/memo/">memo</a> <a class="post-tag" href="/tags/meta-rl/">Meta-RL</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ASAC/"><div class="card-body"> <span class="timeago small" > Jun 27, 2021 <i class="unloaded">2021-06-27T01:50:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications</h3><div class="text-muted small"><p> 목차 Introduction Automating Entropy Adjustment for Maximum Entropy RL Algorithm Reference Introduction 이전 논문인 SAC에서 문제인 고정된 temperature의 문제를 해결하는 부분이 추가된 SAC base논문이다. optimal temperatur...</p></div></div></a></div><div class="card"> <a href="/posts/TD3/"><div class="card-body"> <span class="timeago small" > Jul 1, 2021 <i class="unloaded">2021-07-01T01:40:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Fujimoto 2018 ICML) Addressing Function Approximation Error in Actor-Critic Methods</h3><div class="text-muted small"><p> 목차 Introduction Clipped Double Q-Learning for Actor-Critic Target Networks and Delayed Policy Updates Target Policy Smoothing Regularization Algorithm Reference Introduction 이 논문은 D...</p></div></div></a></div><div class="card"> <a href="/posts/MDP/"><div class="card-body"> <span class="timeago small" > Mar 27, 2021 <i class="unloaded">2021-03-27T20:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>1. Markov Decision Process</h3><div class="text-muted small"><p> 목차 시작하기 전에 Markov Process Markov Decision Process Markov chain 용어 정리 Goal Reference 시작하기 전에 Reinforcement Learning(RL)을 공부하기 위해서 여러 책, 자료를 찾아봤고 이해하는 데 가장 큰 도움이 됬던 강의는 유투브 혁펜하임이라는 채...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/PG-sutton/" class="btn btn-outline-primary" prompt="Older"><p>12. (Sutton Nips 1999) Policy Gradient Methods for Reinforcement Learning with Function Approximation</p></a> <a href="/posts/AC/" class="btn btn-outline-primary" prompt="Newer"><p>13. Actor-Critic</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">Jungi Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/single-agent/">Single Agent</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/multi-agent/">Multi Agent</a> <a class="post-tag" href="/tags/hierarchial-rl/">Hierarchial RL</a> <a class="post-tag" href="/tags/anomaly-detection/">Anomaly Detection</a> <a class="post-tag" href="/tags/distributed-rl/">Distributed RL</a> <a class="post-tag" href="/tags/memo/">memo</a> <a class="post-tag" href="/tags/meta-rl/">Meta RL</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://leejungi.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>

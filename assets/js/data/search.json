[ { "title": "(Gong 2019 ICCV) Memorizing Normality to Detect Anomaly; Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection", "url": "/posts/MemAE/", "categories": "Deep learning, Anomaly Detection", "tags": "Deep learning, Anomaly Detection", "date": "2022-02-10 00:00:00 +0900", "snippet": "목차 Introduction Memory-augmented Autoencoder Memory module Memory-based Representation Attention for Memory Addressing Hard Shrinkage for Sparse Addressing Training Experiment ReferenceIntroduction보통의 Anomaly Detection 논문들은 Auto Encoder를 이용하여 Reconstruction Error를 이..." }, { "title": "(Koch 2015 ICML) Siamese Neural Networks for One-shot Image Recognition", "url": "/posts/siamese/", "categories": "Deep learning", "tags": "Deep learning", "date": "2022-02-03 00:00:00 +0900", "snippet": "목차 Introduction Model ReferenceIntroductionFew-shot learning과 constractive learning을 섞어놓은 듯한 논문이다. 내 관점에서는 Few-shot learning 느낌보다는 constractive learning에 좀 더 맞지 않나라고 보인다. 이 논문은 사전에 준비해둔 데이터와 들어오는 데이터를 비교하여 같음 다름을 이용하여 분류하는 방법이다. 구조와 loss function 등 매우 간단하고 아이디어도 매우 간단하다.ModelSiamese의 모델은 아래 그림..." }, { "title": "(Lasse 2018 ICML) Impala; Scalable distributed deep-rl with importance weighted actor-learner architectures", "url": "/posts/IMPALA/", "categories": "Reinforcement Learning, Distributed RL", "tags": "RL, Distributed RL", "date": "2021-09-20 01:00:00 +0900", "snippet": "목차 Introduction IMPALA V-trace ReferenceIntroduction강화학습에는 빠른 학습을 위해서 A3C와 같은 off-policy based distributed learning 방식을 사용한다. A3C는 worker들 간의 gradient을 공유하여 central parameter server를 update하는 방식이다. IMPALA는 experience의 trajectory를 공유를 하여 centralised learner를 학습시킨다.IMPALA기존 learner policy의 upda..." }, { "title": "(Zong 2018 ICLR) Deep Autoencoding Gaussian Mixture Model For Unsupervised Anomaly Detection", "url": "/posts/DAGMM/", "categories": "Deep learning, Anomaly Detection", "tags": "Deep learning, Anomaly Detection", "date": "2021-09-10 00:00:00 +0900", "snippet": "목차 Introduction Deep Autoencoding Gaussian Mixture Model Experiment ReferenceIntroductionanomaly detection은 high dimension data를 이용하여 low dimension으로 reduction을 하고 거기서 의미 있는 정보를 추출해야한다. 하지만 이는 suboptimal로 수렴할 수 있으며 이 문제를 해결하기 위해서 autoencoder를 이용하여 low dimension representation과 reconstruction ..." }, { "title": "(Hinton 2017 Nips) Dynamic Routing Between Capsules", "url": "/posts/CapsNet/", "categories": "Deep learning", "tags": "Deep learning", "date": "2021-08-17 03:00:00 +0900", "snippet": "목차 Introduction How the vector inputs and outputs of a capsule are computed Margin loss for digit existence CapsNet architecture Comments ReferenceIntroduction이 논문은 일반적인 CNN의 문제점을 해결하기 위해 새로운 네트워크 구조인 Capsule Network를 제안한다. CNN은 simple feature부터 complex feature까지 점차적으로 바뀐다. 그리고 convolution연..." }, { "title": "(Duan 2017 ICLR) RL2; Fast Reinforcement Learning Via Slow Reinforcement Learning", "url": "/posts/RL2/", "categories": "Reinforcement Learning, Meta-RL", "tags": "RL, Meta-RL", "date": "2021-08-01 22:00:00 +0900", "snippet": "목차 Introduction Method Experiment ReferenceIntroduction일반적으로 사람들이 일을 배울 때 이전의 경험을 살려서 더 쉽게 일을 배울 수 있다. 하지만 RL에는 이러한 prior 정보가 부족해서 처음부터 새롭게 경험을 쌓아햐해서 많은 경험이 필요하다. 이러한 문제를 해결하기 위해서 이 논문에서는 Meta-learning을 활용하고 multi-armed bandits 문제와 tabular MDP문제에 대해서 실험을 진행하였다.Method이전 single agent 논문의 object..." }, { "title": "(Ofir 2018 Nips) Data-Efficient Hierarchical Reinforcement Learning", "url": "/posts/HIRO/", "categories": "Reinforcement Learning, Hierarchial RL", "tags": "RL, Hierarchial RL", "date": "2021-07-25 23:00:00 +0900", "snippet": "목차 Introduction Hierachy of Two Policies Parameterized Rewards Off-Policy Corrections for Higher-Level Training Experiment ReferenceIntroduction이 논문의 해결하고자 하는 문제는 세 가지 질문으로 압축할 수 있다. How should one train the lower-level policy to induce semantically distinct behavior? 어떻게 lower-le..." }, { "title": "(Vezhnevets 2017 ICML) Feudal networks for hierarchical reinforcement learning", "url": "/posts/FUN/", "categories": "Reinforcement Learning, Hierarchial RL", "tags": "RL, Hierarchial RL", "date": "2021-07-08 01:00:00 +0900", "snippet": "목차 Introduction FeUdal Networks Learning Experiment ReferenceIntroductionSparse reward에 대한 문제를 해결하기 위해 Hierarchical Reinforcement Learning 방법이 연구되고 있다. 이전에 적은 STRAW논문이 하나의 예시 논문이고 이 Feudal networks 논문 또한 같은 문제를 해결하기 위한 논문이다. 이 논문은 Feudal, 계층적 시스템을 이용한 논문이다. 이전 1993에 Feudal RL를 응용한 논문이다. 계층적으..." }, { "title": "(Fujimoto 2018 ICML) Addressing Function Approximation Error in Actor-Critic Methods", "url": "/posts/TD3/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-07-01 01:40:00 +0900", "snippet": "목차 Introduction Clipped Double Q-Learning for Actor-Critic Target Networks and Delayed Policy Updates Target Policy Smoothing Regularization Algorithm ReferenceIntroduction이 논문은 DDPG base로 발전한 논문이다. 이전에 많이 발생하였던 over estimation 문제를 해결하기 위해 3가지 방법을 제시한다. overestimation bias 문제는 suboptimal po..." }, { "title": "(Haarnoja 2019 arxiv) Soft Actor-Critic Algorithms and Applications", "url": "/posts/ASAC/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-06-27 01:50:00 +0900", "snippet": "목차 Introduction Automating Entropy Adjustment for Maximum Entropy RL Algorithm ReferenceIntroduction이전 논문인 SAC에서 문제인 고정된 temperature의 문제를 해결하는 부분이 추가된 SAC base논문이다. optimal temperature를 찾는 문제는 쉽지 않은 문제이다. 이를 고정해서 사용하게 된다면 학습 결과가 안 좋게 나올 수 있다. 그래서 optimal action의 uncertainty에 따라 temperature를 유..." }, { "title": "(Haarnoja 2018 ICML) Soft Actor-Critic; Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "url": "/posts/SAC/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-06-26 01:50:00 +0900", "snippet": "목차 Introduction Maximum Entropy Derivation of Soft Policy Iteration Soft Actor-Critic Algorithm Enforcing Action Bounds ReferenceIntroduction현실에서 강화학습을 적용하기 어려운 이유는 두 가지가 있다. 첫번째는 task 최적화를 위해서 많은 sample 관측이 필요하다는 것이다. 두번째는 좋은 결과를 위해서 hyperparameter의 조정이 어렵다는 것이다. 한가지 sample efficiency가 떨어..." }, { "title": "(Lillicrap 2015 ICLR) Continuous Control With Deep Reinforcement Learning", "url": "/posts/DDPG/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-06-20 23:50:00 +0900", "snippet": "목차 Introduction Contribution Stochastic vs Deterministic policy Deterministic policy Additional Algorithm ReferenceIntroduction이 논문은 DQN을 응용한 논문이다. DQN은 low-dimensional action space에 관한 문제들만 해결이 가능하다. 즉 사람의 팔과 같은 7 degree of freedom을 가지는 문제의 경우 높은 차원을 가지고 있기 때문에 해결하기가 어렵다. 그리고 Continuous s..." }, { "title": "(Wu 2017 arxiv) Scalable trust-region method for deep reinforcementlearning using Kronecker-factored approximation", "url": "/posts/ACKTR/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-06-18 01:30:00 +0900", "snippet": "목차 Introduction Natural gradient using Kronecker-factored approximation ReferenceIntroduction이 논문은 TRPO에서 파생되었다. 이 논문의 주된 contribution은 TRPO의 computation time과 sample efficiency를 해결하기 위해 Kronecker-factored approximation을 사용한다는 것이다. 저자는 처음으로 scalable trust region natural graident method를 사용했다고 ..." }, { "title": "(ICLR 2017 Wang) Sample Efficient Actor-Critic with Experience Replay", "url": "/posts/ACER/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-06-14 02:30:00 +0900", "snippet": "목차 Introduction Discrete Actor Critic with Experience Replay Multi-Step Estimation of the State-Action Value Fucntion Importance Weight Truncation With Bias Correction Efficient Trust Region Policy Optimization Algorithm ReferenceIntroduction학습을 위해서 Sampling을 하는 것은 중요..." }, { "title": "(Nips 2016 Vezhnevets) Strategic Attentive Writer for Learning Macro-Actions", "url": "/posts/STRAW/", "categories": "Reinforcement Learning, Hierarchial RL", "tags": "RL, Hierarchial RL", "date": "2021-06-06 00:23:00 +0900", "snippet": "목차 Introduction The state of the network Attentive planning Action-plan update Commitment-plan update Learning Experiment ReferenceIntroduction현재 많은 RL 논문들이 나왔고 이들은 low-level의 행동의 결과로 high-level의 행동을 이끌어내고 있지만 이 low, high level 행동의 term이 길어진다면 문제가 발생한다. 즉 sparse reward 형태가 된다면 문제가 발생한다. 그..." }, { "title": "(Gregor 2015 ICML) DRAW; A Recurrent Neural Network For Image Generation", "url": "/posts/DRAW/", "categories": "Deep learning", "tags": "Deep learning", "date": "2021-06-04 00:09:00 +0900", "snippet": "목차 Introduction DRAW Architecture Read and Write operations ReferenceIntroduction이 논문은 generative model 논문으로 VAE와 비슷한 느낌이다. 하지만 VAE와 다르게 RNN 구조가 추가된다. 왜 RNN 구조가 추가 되었나? 사람들이 그림을 그릴 때에서 아이디어를 가져왔다고 한다. 사람들이 그림을 그릴 때 전체를 한번에 그리는 것이 아닌 일부분에 집중을 하여 점차적으로 큰 그림을 그린다. 이러한 구조가 RNN처럼 time series로 볼 수 ..." }, { "title": "16. (Schulman 2017 arxiv) Proximal Policy Optimization Algorithms", "url": "/posts/PPO/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-23 04:00:00 +0900", "snippet": "목차 Introduction Clipped Surrogate Objective Adaptive KL Penalty Coefficient Algorithm Generalized Advantage Estimation Entropy Experiment ReferenceIntroduction이전 TRPO를 contraints 대신 penalty로 update하기에는 \\(\\beta\\)를 결정하기 힘들다는 문제가 있다. 하지만 \\(\\beta\\)를 간단하게 선택하여 업데이트 하는 방법은 pol..." }, { "title": "15. (Schulman 2017 ICML) Trust Region Policy Optimization", "url": "/posts/TRPO/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-16 20:00:00 +0900", "snippet": "목차 Preliminaries Monotonic Improvement Guarantee for General Stochastic Policies Optimization of Parameterized Policies Sample-Based Estimation of the Objective and Constraint Training Conjugate Gradient ReferencePreliminariesTrust Region Policy Optimization(TRPO) 논문은 Kakade &amp;amp; Lang..." }, { "title": "14. (Mnih 2016 ICML) Asynchronous Methods for Deep Reinforcement Learning", "url": "/posts/A3C/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-08 10:30:00 +0900", "snippet": "목차 Asynchronous Advantage Actor Critic N-step TD Entropy Algorithm ReferenceAsynchronous Advantage Actor CriticReplay buffer를 사용함으로써 memory와 real interaction마다 computation을 요구하는 문제를 해결하기 위해 asynchronous advantage actor critic 방법을 제안. 이전에 비슷한 방법이 있지만 이 논문에서는 한 cpu에서 multi thread를 이용하는 방법을 사용...." }, { "title": "13. Advanced Actor-Critic(A2C)", "url": "/posts/A2C/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-07 02:20:00 +0900", "snippet": "목차 Advanced Actor Critic Algorithm ReferenceAdvanced Actor Critic\\[\\nabla_\\theta J_\\theta \\simeq \\underset{t=0}{\\overset{\\infty}{\\sum}} \\int_{s_t,a_t} \\nabla_\\theta \\text{ln} P_\\theta (a_t \\vert s_t) Q(s_t, a_t) P_\\theta(s_t, a_t) \\, d s_t,a_t\\]위 식에서 Q 대신 action의 함수가 아닌 \\(b(s_t)\\)를 넣어보자\\[\\unde..." }, { "title": "13. Actor-Critic", "url": "/posts/AC/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-07 02:00:00 +0900", "snippet": "목차 Actor Critic Algorithm ReferenceActor CriticREINFORCEMENT 알고리즘에서 \\(G_t\\) 대신 Q를 이용하는 방법이다. Q를 이용하기 때문에 actor와 critic 두 가지 네트워크를 이용한다.\\[\\nabla_\\theta J_\\theta \\simeq \\int_\\tau \\underset{t=0}{\\overset{\\infty}{\\sum}} \\nabla_\\theta \\text{ln} P_\\theta (a_t | s_t) G_t P_\\theta(\\tau) \\, d \\tau\\]\\(P..." }, { "title": "(Lee 2019 arxiv) Tsallis reinforcement learning; A unified framework for maximum entropy reinforcement learning", "url": "/posts/tac/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-02 22:10:00 +0900", "snippet": "목차 Problem definition Background Shannon-Gibbs entropy q-Expoential,q-Logarithm and Tsallis Entropy Proposition 1 Proposition 2 Bandit with Maximum Tsallis Entropy Proposition 3 Proposition 4 q-Maximum ..." }, { "title": "12. (Sutton Nips 1999) Policy Gradient Methods for Reinforcement Learning with Function Approximation", "url": "/posts/PG-sutton/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-05-01 18:00:00 +0900", "snippet": "목차 Policy Gradient Theorem Theorem 1 Policy Gradient with Approximation Theorem 2 Application to Deriving Algorithms and Advantages ReferencePolicy Gradient Theorem이전의 value function과 deterministic policy가 아닌 stochastic policy를 이용한다. \\(\\theta\\)는 policy parameter, \\(\\rho\\)는 pol..." }, { "title": "11. Policy Gradient", "url": "/posts/PG/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-27 01:30:00 +0900", "snippet": "목차 Policy Gradient REINFORCEMENT Algorithm Expectation form ReferencePolicy Gradient\\[J_{\\theta}=\\int_{\\tau} G_0 P_{\\theta}(\\tau) \\,d\\tau\\]\\[\\theta \\gets \\theta + \\alpha \\nabla_{\\theta}J_{\\theta}\\]policy-based 목적 함수는 위와 같다. trajectory에 대해서 적분을 해야하기 때문에 어려운 점이 있다. 그래서 이를 expectation 형식으로 바꾸어 편..." }, { "title": "10. Policy based", "url": "/posts/policy-based/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-27 01:00:00 +0900", "snippet": "목차 Problem definition Why? Policy-based Policy-based의 목표 ReferenceProblem definition기존 Q learning은 value-based로 \\(\\epsilon\\)-greedy 방법을 사용하였다. policy-based는 neural network로부터 mean, variance를 얻어서 만든 distribution을 따른 action sample을 얻는 방법이다. 그렇다면 왜 policy based를 사용해야하는 가?Why? Policy-basedvalue-b..." }, { "title": "9. Prioritized Experience Replay", "url": "/posts/PER/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-25 18:00:00 +0900", "snippet": "목차 Problem definition Priority Importance Sampling Importance Sampling Weight Algorithm ReferenceProblem definition기존 Experience replay buffer는 uniform하게 선택하여 우선 순위가 없다. 우선 순위를 매기면 state correlation이 높아질 수 있다는 문제가 있다. 그러므로 우선 순위의 장점과 random sample의 장점을 잘 섞어보는 것이 목적이다.PriorityReplay buffer가 ..." }, { "title": "5. (Iqbal 2019 ICML) Actor-Attention-Critic for Multi-Agent Reinforcement Learning", "url": "/posts/MAAC/", "categories": "Reinforcement Learning, Multi-Agent", "tags": "RL, Multi-Agent", "date": "2021-04-22 01:30:00 +0900", "snippet": "목차 Problem definition Background Multi-Actor-Attention-Critic(MAAC) Algorithm Experiment Review comment ReferenceProblem definition이전 Multi-agent Reinforcement learning을 위해서 하는 가장 간단한 방법은 독립적인 agent를 두고 학습하는 방법이지만 이 방법은 환경이 stationary하고 markovian이여야하는 가정을 위반한다. 그래서 다른 방법으로 모든 agent로부터 정보를 ..." }, { "title": "8. Dueling DQN", "url": "/posts/Dueling-DQN/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-19 00:10:00 +0900", "snippet": "목차 Advantage function Dueling DQN Prior information Q value with prior information Algorithm ReferenceAdvantage functionAdvantage function은 Q-V 수식으로 이루어져있다. 수식 자체로 보면 Q 값이 평균보다 얼마나 더 좋거나 나쁜 값을 가지고 있는 지를 나타내는 function이다. 이로 인해서 더 학습이 잘되는 것이라고 볼 수 있다.Dueling DQNDueling DQN의 구조는 아래 그림과 같이 생겼다...." }, { "title": "7. DDQN", "url": "/posts/DDQN/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-18 22:10:00 +0900", "snippet": "목차 Problem definition Double DQN Proof ReferenceProblem definitionDQn의 \\(\\gamma \\operatorname{max} Q\\)는 overestimate되어있다.Theorem 1에 의하면 \\(V^*(s)\\)를 target(optimal Q)라고 했을 시 \\(V^*(s) - \\underset{a}{\\operatorname{max}} Q_t(s,a) \\ge \\sqrt{\\frac{c}{m-1}}\\) 이라는 식이 만들어진다. 이 식은 DQN이 항상 \\(\\sqrt{\\frac..." }, { "title": "6. DQN", "url": "/posts/DQN/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-18 21:30:00 +0900", "snippet": "목차 13DQN 15DQN Reference13DQNAtari 게임과 같이 state가 많은 곳에서 Q-table을 사용하기가 어렵기 때문에 DNN을 이용하여 해결한다. DNN의 output이 Q값이 나오고 Q의 1 step TD를 이용하여 network parameter를 update한다.13 DQN의 contribution은 3가지 이다. CNN 이용 Frame skipping 게임에서 바로 다음 frame은 이전 frame과 매우 비슷한 값을 가지기 때문에 state correlation이 높다는..." }, { "title": "4. (Du 2019 NIPS) LIIR; Learning Individual Intrinsic Reward in Multi-Agent Reinforcement learning", "url": "/posts/LIIR/", "categories": "Reinforcement Learning, Multi-Agent", "tags": "RL, Multi-Agent", "date": "2021-04-18 01:11:00 +0900", "snippet": "목차 Problem definition Background The Objective Algorithm Experiment Code review Review comment ReferenceProblem definitionCOMA와 QMIX와 같은 논문들은 critic의 구조에 집중하여 방법론을 적용하였으며 value function에 assumption을 적용하였다는 문제가 있다. 이 논문에서는 assumption이 없고 즉각적인 intrinsic reward를 보여줄 수 있다는 장점이 있다고한다. 이 논문에서는 ..." }, { "title": "3. (Rashid 2018 ICML) Qmix; Monotonic value function factorisation for deep multi-agent reinforcement learning", "url": "/posts/QMIX/", "categories": "Reinforcement Learning, Multi-Agent", "tags": "RL, Multi-Agent", "date": "2021-04-11 02:10:00 +0900", "snippet": "목차 Problem definition Related work QMIX Representational Complexity Experiment Review comment ReferenceProblem definitionAgent의 적절한 action을 위해서 centralised action-value function인 \\(Q_{tot}\\)이 필요하다. 하지만 많은 agent가 있거나 만약에 \\(Q_{tot}\\) 얻더라도 이를 다시 각각의 agent에 사용하기 위해 각각의 observation을 기반으로한 행동을 ..." }, { "title": "5. On/off-policy", "url": "/posts/On-off-policy/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-10 21:40:00 +0900", "snippet": "목차 Behavior policy and target policy On policy Off policy Off policy의 장점 ReferenceBehavior policy and target policyOn policy와 off policy는 behavior policy와 target policy가 같은지 다른지에 따라 결정이된다. Behavior policy는 현재 action을 고르기 위해서 사용된 policy이다.Target policy는 TD target을 만들기 위해서 사용된 policy이다.On poli..." }, { "title": "4. Monte Carlo and Temporal Difference", "url": "/posts/MC-TD/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-10 15:30:00 +0900", "snippet": "목차 In previous posting Monte Carlo Temporal Difference MC와 TD 비교 ReferenceIn previous posting이전 posting에서 \\(Q=Q^*\\)라는 가정이 있다면 \\(P(a_t \\mid s_t)\\)를 구하면 expected return을 최대화할 수 있다고 하였다. 이 posting에서는 \\(Q=Q^*\\)를 만족하기 위해 이를 구하는 방법에 대해서 얘기할 것이다.Monte Carlo(MC)우리는 \\(Q^*\\)를 구하기 위해서 training을 할 것이다.M..." }, { "title": "3. Optimal Policy", "url": "/posts/Optimal-policy/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-10 03:30:00 +0900", "snippet": "목차 In previous posting Assumption Optimal policy ReferenceIn previous posting이전 posting에서 \\(V(s_t)\\)에 대해서 정의를 하고 이를 \\(Q(s_t)\\)로 표현하였다.\\(V(s_t) = \\int_{a_t} Q(s_t,a_t)P(a_t \\mid s_t) \\, da_t\\)Assumption\\(Q=Q^*\\)라면 \\(P^*(a_t \\mid s_t)\\)만 구하면 expected return을 최대화할 수 있다.Q에는 \\(P(a_{t+1} \\mid s_{t+..." }, { "title": "2. (Foerster 2017 AAAI) Counterfactual Multi-Agent Policy Gradients", "url": "/posts/COMA/", "categories": "Reinforcement Learning, Multi-Agent", "tags": "RL, Multi-Agent", "date": "2021-04-04 04:30:00 +0900", "snippet": "목차1. Main ideas2. Notation3. Indepedent Actor-Critic4. Counterfactual Multi-Agent Policy Gradients5. COMA Lemma 16. Algorithm7. ReferenceMain ideas Centralised critic 사용 Counterfactual baseline 사용 Critic representation 사용Notation Stochastic game \\(G = &amp;lt;S,U,P,r,Z,O,n, \\gamma &amp;gt;\\) ..." }, { "title": "2. Bellman Equation", "url": "/posts/Bellman/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-04-03 16:16:00 +0900", "snippet": "목차 Bellman Equation ReferenceBellman Equation 벨만 방정식이란 어떤 상태의 값과 그 후에 이어지는 상태의 값들 사이의 관계를 표현한다. 그래서 state value function이 reward가 재귀적으로 사용되므로 \\(V(s_t)\\)를 \\(V(S_{t+1})\\)로 표현할 수 있다. \\(Q(s_t,a_t)\\)로 \\(V(s_t)\\) 표현하기\\(V(s_t) \\doteq \\int_{a_{t}:a_{\\infty}} G_tP(a_{t+1},s_{t+2},a_{t+2} \\cdots..." }, { "title": "1. (Tan 1993 ICML) Multi-Agent Reinforcement Learning; Independent vs Cooperative Agents", "url": "/posts/MARL-1993/", "categories": "Reinforcement Learning, Multi-Agent", "tags": "RL, Multi-Agent", "date": "2021-03-28 01:27:00 +0900", "snippet": "목차1. Introduction2. 환경3. Case1: sharing sensation4. Case2: sharing policies or episodes5. Case3: on joint tasks6. Concolusion7. ReferenceIntroduction사람들이 일을 하기 위해서는 분업을 하고 있고 곤충들 또한 그렇다. 예를 들어 개미들은 큰 일을 하기 위해서 식량을 옮기는 개미와, 식량을 모으는 개미처럼 각각의 다른 일을 하여 하나의 큰 일을 완료하고자 한다. 현재 RL은 단일 agent에 대해서만 적용하고 있지..." }, { "title": "1. Markov Decision Process", "url": "/posts/MDP/", "categories": "Reinforcement Learning, Single-Agent", "tags": "RL, Single-Agent", "date": "2021-03-27 20:00:00 +0900", "snippet": "목차 시작하기 전에 Markov Process Markov Decision Process Markov chain 용어 정리 Goal Reference시작하기 전에Reinforcement Learning(RL)을 공부하기 위해서 여러 책, 자료를 찾아봤고 이해하는 데 가장 큰 도움이 됬던 강의는 유투브 혁펜하임이라는 채널에서 만든 강화학습 강의였다. 이 single-agent 시리즈 포스팅은 해당 유투브를 통해 얻은 지식을 바탕으로 정리를 할 것이다. 하지만 이 강의와 책들의 표기 방식이 다르기 때문에 책들의 표기 ..." }, { "title": "Goal", "url": "/posts/post/", "categories": "Memo", "tags": "memo", "date": "2021-03-27 18:47:00 +0900", "snippet": "왜 블로그를 쓰는건가? 정리 논문을 읽고 공부한 것을 글로 정리하며 다시 리마인드를 목표로 하고 있다. 방지 지속적인 포스팅을 통하여 게을러지는 것을 방지하고자 한다. 방향성강화학습 논문 및 구현을 중심으로 포스팅할 것이다.목표현재 많이 알려진 강화학습 논문 리딩 및 구현을 하고 있다. 이를 정리한 후 새로운 강화학습 논문 리딩 후 포스팅을 할 예정이다.단기 목표일주일 동안 이론 정리 1, 논문 1개를 포스팅을 할 예정이며 이후 논문 1개 포스팅 및 논문 구현하는 과정 및..." } ]
